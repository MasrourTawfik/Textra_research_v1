{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, List, Optional\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PATH = Path(r\"data\\raw\\BartoSutton.pdf\") # add your own path\n",
    "PROCESSED_DIR = Path(\"data/processed\")\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookParser:\n",
    "    def __init__(self):\n",
    "        # Pattern for main chapter starts\n",
    "        self.chapter_pattern = re.compile(r'^Chapter\\s+(\\d+)\\s*$\\s*([^\\n]+)', re.MULTILINE)\n",
    "        \n",
    "        # Pattern for sections \n",
    "        self.section_pattern = re.compile(r'^(\\d+\\.\\d+)\\s+([^\\n]+)')\n",
    "        \n",
    "        # Pattern to remove headers/footers (matches format from Image 3)\n",
    "        self.header_pattern = re.compile(r'^\\d+\\s+Chapter \\d+:.+$', re.MULTILINE)\n",
    "        \n",
    "        # Pattern to identify part headers (like \"I Tabular Solution Methods\")\n",
    "        self.part_pattern = re.compile(r'^(I{1,3}|IV)\\s+([^\\n]+)$', re.MULTILINE)\n",
    "        \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Remove headers, footers, and extra whitespace\"\"\"\n",
    "        # Remove headers/footers\n",
    "        text = self.header_pattern.sub('', text)\n",
    "        # Remove multiple spaces and normalize newlines\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def parse_chapter(self, text: str, chapter_num: int) -> Dict:\n",
    "        \"\"\"Parse a single chapter's content and structure\"\"\"\n",
    "        sections = {}\n",
    "        current_section = None\n",
    "        section_content = []\n",
    "        \n",
    "        # Split into lines and process\n",
    "        lines = text.split('\\n')\n",
    "        for line in lines:\n",
    "            section_match = self.section_pattern.match(line)\n",
    "            if section_match:\n",
    "                # Save previous section\n",
    "                if current_section:\n",
    "                    sections[current_section[0]] = {\n",
    "                        'title': current_section[1],\n",
    "                        'content': '\\n'.join(section_content)\n",
    "                    }\n",
    "                # Start new section\n",
    "                current_section = (section_match.group(1), section_match.group(2))\n",
    "                section_content = [line]\n",
    "            elif current_section:\n",
    "                section_content.append(line)\n",
    "        \n",
    "        # Don't forget last section\n",
    "        if current_section:\n",
    "            sections[current_section[0]] = {\n",
    "                'title': current_section[1],\n",
    "                'content': '\\n'.join(section_content)\n",
    "            }\n",
    "            \n",
    "        return sections\n",
    "\n",
    "    def process_book(self, text: str) -> Dict[int, Dict]:\n",
    "        \"\"\"Process entire book\"\"\"\n",
    "        chapters = {}\n",
    "        \n",
    "        # Find all chapter starts\n",
    "        for match in self.chapter_pattern.finditer(text):\n",
    "            chapter_num = int(match.group(1))\n",
    "            chapter_title = match.group(2).strip()\n",
    "            \n",
    "            # Get chapter content\n",
    "            start_pos = match.start()\n",
    "            end_pos = len(text)\n",
    "            # Find next chapter start if exists\n",
    "            next_match = self.chapter_pattern.search(text, pos=match.end())\n",
    "            if next_match:\n",
    "                end_pos = next_match.start()\n",
    "                \n",
    "            chapter_content = text[start_pos:end_pos]\n",
    "            \n",
    "            # Parse chapter sections\n",
    "            sections = self.parse_chapter(chapter_content, chapter_num)\n",
    "            \n",
    "            chapters[chapter_num] = {\n",
    "                'title': chapter_title,\n",
    "                'content': self.clean_text(chapter_content),\n",
    "                'sections': sections\n",
    "            }\n",
    "        \n",
    "        return chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_book():\n",
    "    # Read PDF\n",
    "    print(\"Reading PDF...\")\n",
    "    with open(PDF_PATH, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    \n",
    "    # Process book\n",
    "    parser = BookParser()\n",
    "    chapters = parser.process_book(text)\n",
    "    \n",
    "    # Save results\n",
    "    for chapter_num, chapter_data in sorted(chapters.items()):\n",
    "        # Save chapter content\n",
    "        chapter_file = PROCESSED_DIR / f\"chapter_{chapter_num:02d}.txt\"\n",
    "        with open(chapter_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(chapter_data['content'])\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata_file = PROCESSED_DIR / f\"chapter_{chapter_num:02d}_meta.json\"\n",
    "        metadata = {\n",
    "            'title': chapter_data['title'],\n",
    "            'sections': {num: data['title'] for num, data in chapter_data['sections'].items()}\n",
    "        }\n",
    "        with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nChapter {chapter_num}: {chapter_data['title']}\")\n",
    "        print(\"Sections:\")\n",
    "        for section_num, section_data in sorted(chapter_data['sections'].items()):\n",
    "            print(f\"  {section_num} {section_data['title']}\")\n",
    "            \n",
    "    return chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading PDF...\n",
      "\n",
      "Chapter 1: Introduction\n",
      "Sections:\n",
      "  1.1 Reinforcement Learning\n",
      "  1.2 The example of Phil’s breakfast in this chapter was inspired by Agre (1988).\n",
      "  1.3 Elements of Reinforcement Learning\n",
      "  1.4 Limitations and Scope\n",
      "  1.5 The temporal-di↵erence method used in the tic-tac-toe example is developed in\n",
      "  1.6 Summary\n",
      "  1.7 Early History of Reinforcement Learning\n",
      "\n",
      "Chapter 2: Multi-armed Bandits\n",
      "Sections:\n",
      "  2.1 Bandit problems have been studied in statistics, engineering, and psychology. In\n",
      "  2.10 Bellman (1956) was the ﬁrst to show how dynamic programming could be used\n",
      "  2.2 Action-value methods for our k-armed bandit problem were ﬁrst proposed by\n",
      "  2.3 The 10-armed Testbed\n",
      "  2.4 Incremental Implementation\n",
      "  2.5 Tracking a Nonstationary Problem\n",
      "  2.6 Optimistic initialization was used in reinforcement learning by Sutton (1996).\n",
      "  2.7 Early work on using estimates of the upper conﬁdence bound to select actions\n",
      "  2.8 Gradient bandit algorithms are a special case of the gradient-based reinforcement\n",
      "  2.9 The term associative search and the corresponding problem were introduced by\n",
      "\n",
      "Chapter 3: Finite Markov Decision\n",
      "Sections:\n",
      "  3.1 Our characterization of the dynamics of an MDP in terms of p(s0,r|s, a)i s\n",
      "  3.2 The reward hypothesis was suggested by Michael Littman (personal communica-\n",
      "  3.3 Returns and Episodes\n",
      "  3.4 Uniﬁed Notation for Episodic and Continuing Tasks\n",
      "  3.5 Policies and Value Functions\n",
      "  3.6 Optimal Policies and Optimal Value Functions\n",
      "  3.7 Optimality and Approximation\n",
      "  3.8 Summary\n",
      "\n",
      "Chapter 4: Dynamic Programming\n",
      "Sections:\n",
      "  4.1 Policy Evaluation (Prediction)\n",
      "  4.2 Policy Improvement\n",
      "  4.3 Policy Iteration\n",
      "  4.4 Value Iteration\n",
      "  4.5 Asynchronous DP algorithms are due to Bertsekas (1982, 1983), who also called\n",
      "  4.6 Generalized Policy Iteration\n",
      "  4.7 This section, written with the help of Michael Littman, is based on Littman,\n",
      "  4.8 Summary\n",
      "\n",
      "Chapter 5: Monte Carlo Methods\n",
      "Sections:\n",
      "  5.1 Monte Carlo Prediction\n",
      "  5.10 Summary\n",
      "  5.2 Monte Carlo Estimation of Action Values\n",
      "  5.3 Monte Carlo Control\n",
      "  5.4 Monte Carlo Control without Exploring Starts\n",
      "  5.5 E\u0000cient o↵-policy learning has become recognized as an important challenge\n",
      "  5.6 Incremental Implementation\n",
      "  5.7 The racetrack exercise is adapted from Barto, Bradtke, and Singh (1995), and\n",
      "  5.8 Our treatment of the idea of discounting-aware importance sampling is based on\n",
      "  5.9 Per-decision importance sampling was introduced by Precup, Sutton, and Singh\n",
      "\n",
      "Chapter 6: Temporal-Di↵erence Learning\n",
      "Sections:\n",
      "  6.1 TD Prediction\n",
      "  6.2 Advantages of TD Prediction Methods\n",
      "  6.3 The optimality of the TD algorithm under batch training was established by\n",
      "  6.4 The Sarsa algorithm was introduced by Rummery and Niranjan (1994). They\n",
      "  6.5 Q-learning was introduced by Watkins (1989), whose outline of a convergence\n",
      "  6.6 The Expected Sarsa algorithm was introduced by George John (1994), who\n",
      "  6.7 Maximization bias and double learning were introduced and extensively investi-\n",
      "  6.8 The notion of an afterstate is the same as that of a “post-decision state” (Van\n",
      "  6.9 Summary\n",
      "\n",
      "Chapter 7: n-step Bootstrapping\n",
      "Sections:\n",
      "  7.1 n-step TD Prediction\n",
      "  7.2 n-step Sarsa\n",
      "  7.3 n-step O↵-policy Learning\n",
      "  7.4 *Per-decision Methods with Control Variates\n",
      "  7.5 O↵-policy Learning Without Importance Sampling:\n",
      "  7.6 TheQ(\u0000) algorithm is new to this text, but closely related algorithms have been\n",
      "  7.7 Summary\n",
      "\n",
      "Chapter 8: Planning and Learning with\n",
      "Sections:\n",
      "  8.1 The overall view of planning and learning presented here has developed gradually\n",
      "  8.10 Abramson’s (1990) expected-outcome model is a rollout algorithm applied to two-\n",
      "  8.11 The central ideas of MCTS were introduced by Coulom (2006) and by Kocsis\n",
      "  8.12 Summary of the Chapter\n",
      "  8.13 Summary of Part I: Dimensions\n",
      "  8.2 The terms direct and indirect , which we use to describe di↵erent kinds of\n",
      "  8.3 There have been several works with model-based reinforcement learning that take\n",
      "  8.4 Prioritized sweeping was developed simultaneously and independently by Moore\n",
      "  8.5 This section was strongly inﬂuenced by the experiments of Singh (1993).\n",
      "  8.6 Trajectory Sampling\n",
      "  8.7 Real-time Dynamic Programming\n",
      "  8.8 Planning at Decision Time\n",
      "  8.9 For further reading on heuristic search, the reader is encouraged to consult texts\n",
      "\n",
      "Chapter 9: On-policy Prediction with\n",
      "Sections:\n",
      "  9.1 Value-function Approximation\n",
      "  9.10 The origin of kernel regression is the method of potential functions of Aizerman,\n",
      "  9.11 For Emphatic-TD methods, see the bibliographical notes to Section 11.8.\n",
      "  9.12 Summary\n",
      "  9.2 The Prediction Objective ( VE)\n",
      "  9.3 Gradient-descent methods for minimizing mean-squared error in supervised\n",
      "  9.4 Sutton (1988) proved convergence of linear TD(0) in the mean to the minimal\n",
      "  9.5 Our presentation of the range of possibilities for linear function approximation is\n",
      "  9.6 The introduction of the threshold logic unit as an abstract model neuron by\n",
      "  9.7 Nonlinear Function Approximation:\n",
      "  9.8 LSTD is due to Bradtke and Barto (see Bradtke, 1993, 1994; Bradtke and Barto,\n",
      "  9.9 Our discussion of memory-based function approximation is largely based on\n",
      "\n",
      "Chapter 10: On-policy Control with\n",
      "Sections:\n",
      "  10.1 Semi-gradient Sarsa with function approximation was ﬁrst explored by Rummery\n",
      "  10.2 Episodic n-step semi-gradient Sarsa is based on the forward Sarsa( \u0000) algorithm\n",
      "  10.3 The average-reward formulation has been described for dynamic programming\n",
      "  10.4 The recognition of the limitations of discounting as a formulation of the rein-\n",
      "  10.5 Di↵erential Semi-gradient n-step Sarsa\n",
      "  10.6 Summary\n",
      "\n",
      "Chapter 11: *O↵-policy Methods with\n",
      "Sections:\n",
      "  11.1 The ﬁrst semi-gradient method was linear TD( \u0000) (Sutton, 1988). The name\n",
      "  11.10 Summary\n",
      "  11.2 The earliest w-to-2 wexample was given by Tsitsiklis and Van Roy (1996), who\n",
      "  11.3 The deadly triad was ﬁrst identiﬁed by Sutton (1995b) and thoroughly analyzed\n",
      "  11.4 This kind of linear analysis was pioneered by Tsitsiklis and Van Roy (1996; 1997),\n",
      "  11.5 TheBEwas ﬁrst proposed as an objective function for dynamic programming by\n",
      "  11.6 The contents of this section are new to this text.\n",
      "  11.7 Gradient-TD methods were introduced by Sutton, Szepesv´ ari, and Maei (2009b).\n",
      "  11.8 Emphatic-TD methods were introduced by Sutton, Mahmood, and White (2016).\n",
      "  11.9 Reducing Variance\n",
      "\n",
      "Chapter 12: Eligibility Traces\n",
      "Sections:\n",
      "  12.1 Compound updates were called “complex backups” in the ﬁrst edition of this\n",
      "  12.10 Watkins’s Q( \u0000) is due to Watkins (1989). The tabular, episodic, o✏ine version\n",
      "  12.11 GTD( \u0000) is due to Maei (2011). GQ( \u0000) is due to Maei and Sutton (2010).\n",
      "  12.12 Implementation Issues\n",
      "  12.13 Conclusions\n",
      "  12.2 TD(\u0000) with accumulating traces was introduced by Sutton (1988, 1984). Con-\n",
      "  12.3 Truncated TD methods were developed by Cichosz (1995) and van Seijen (2016).\n",
      "  12.4 The idea of redoing updates was extensively developed by van Seijen, originally\n",
      "  12.5 True online TD( \u0000) is primarily due to Harm van Seijen (van Seijen and Sutton,\n",
      "  12.6 The material in this section is from van Hasselt and Sutton (2015).\n",
      "  12.7 Sarsa( \u0000) with accumulating traces was ﬁrst explored as a control method by\n",
      "  12.8 Perhaps the ﬁrst published discussion of variable \u0000was by Watkins (1989), who\n",
      "  12.9 O↵-policy eligibility traces were introduced by Precup et al. (2000, 2001), then\n",
      "  9.11 and 11.8) to eligibility traces. The resultant algorithm retains strong o↵-policy\n",
      "\n",
      "Chapter 13: Policy Gradient Methods\n",
      "Sections:\n",
      "  13.1 Example 13.1 and the results with it in this chapter were developed with Eric\n",
      "  13.2 The policy gradient theorem here and on page 334 was ﬁrst obtained by Marbach\n",
      "  13.3 REINFORCE is due to Williams (1987, 1992). Phansalkar and Thathachar\n",
      "  13.4 The baseline was introduced in Williams’s (1987, 1992) original work. Greensmith,\n",
      "  13.5 Actor–Critic Methods\n",
      "  13.6 Policy Gradient for Continuing Problems\n",
      "  13.7 The ﬁrst to show how continuous actions could be handled this way appears\n",
      "  13.8 Summary\n",
      "\n",
      "Chapter 14: Psychology\n",
      "Sections:\n",
      "  14.1 Dayan, Niv, Seymour, and Daw (2006) focused on interactions between clas-\n",
      "  14.2 Classical Conditioning\n",
      "  14.3 Section 1.7 includes comments on the history of trial-and-error learning and\n",
      "  14.4 Spence, Hull’s student and collaborator at Yale, elaborated the role of higher-\n",
      "  14.5 Thistlethwaite (1951) provides an extensive review of latent learning experiments\n",
      "  14.6 Connections between habitual and goal-directed behavior and model-free and\n",
      "  14.7 Summary\n",
      "\n",
      "Chapter 15: Neuroscience\n",
      "Sections:\n",
      "  15.1 There are many good expositions of basic neuroscience. Kandel, Schwartz, Jessell,\n",
      "  15.10 Research on the behavior of reinforcement learning agents in team and game\n",
      "  15.11 Yin and Knowlton (2006) reviewed ﬁndings from outcome-devaluation experi-\n",
      "  15.12 Keiﬂin and Janak (2015) reviewed connections between TD errors and addiction.\n",
      "  15.13 Summary\n",
      "  15.2 Berridge and Kringelbach (2008) reviewed the neural basis of reward and pleasure,\n",
      "  15.3 The reward prediction error hypothesis of dopamine neuron activity is most\n",
      "  15.4 Graybiel (2000) is a brief primer on the basal ganglia. The experiments mentioned\n",
      "  15.5 Schultz’s 1998 survey article is a good entr´ ee into the very extensive literature\n",
      "  15.6 This section roughly follows Barto (1995a) in explaining how TD errors mimic the\n",
      "  15.7 This section is largely based on Takahashi, Schoenbaum, and Niv (2008) and Niv\n",
      "  15.8 The actor learning rule discussed here is more complicated than the one in\n",
      "  15.9 Klopf’s hedonistic neuron hypothesis (Klopf 1972, 1982) inspired our actor–critic\n",
      "\n",
      "Chapter 16: Applications and Case Studies\n",
      "Sections:\n",
      "  16.1 TD-Gammon\n",
      "  16.2 Samuel’s Checkers Player\n",
      "  16.3 Watson’s Daily-Double Wagering\n",
      "  16.4 Optimizing Memory Control\n",
      "  16.5 Human-level Video Game Play\n",
      "  16.6 Mastering the Game of Go\n",
      "  16.7 Personalized Web Services\n",
      "  16.8 Thermal Soaring\n",
      "  2.5 minutes simulated with a 1 second time step. Learning e↵ectively converged after a\n",
      "\n",
      "Chapter 17: Frontiers\n",
      "Sections:\n",
      "  17.1 General value functions were ﬁrst explicitly identiﬁed by Sutton and colleagues\n",
      "  17.2 The formalization of temporally abstract courses of action as options was intro-\n",
      "  17.3 A good presentation of the POMDP approach is given by Monahan (1982). PSRs\n",
      "  17.4 Early e↵orts to include advice and teaching in reinforcement learning include\n",
      "  17.5 We recommend the book by Goodfellow, Bengio, and Courville (2016) for discus-\n",
      "  17.6 The Future of Artiﬁcial Intelligence\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    chapters = process_book()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the chapters are separated correctly but the sections titles are not, we will fix that problem in the upcoming cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaner:\n",
    "    def __init__(self):\n",
    "        # Patterns for cleaning\n",
    "        self.equation_pattern = re.compile(r'NULL?:?[A-Za-z0-9\\(\\)\\[\\]\\{\\}\\+\\-\\*\\/\\=\\>\\<\\!\\~\\#\\$\\%\\^\\&\\_\\|\\.\\,\\;\\:]+')\n",
    "        self.null_pattern = re.compile(r'NULL\\s*')\n",
    "        self.multiple_spaces = re.compile(r'\\s+')\n",
    "        self.header_footer = re.compile(r'^\\d+\\s*Chapter \\d+:.+$', re.MULTILINE)\n",
    "        \n",
    "    def clean_equations(self, text: str) -> str:\n",
    "        \"\"\"Clean equations and mathematical expressions\"\"\"\n",
    "        # Replace equation-like patterns with [EQUATION]\n",
    "        text = self.equation_pattern.sub(' [EQUATION] ', text)\n",
    "        return text\n",
    "        \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Main cleaning function\"\"\"\n",
    "        # Remove NULL markers\n",
    "        text = self.null_pattern.sub('', text)\n",
    "        \n",
    "        # Clean equations\n",
    "        text = self.clean_equations(text)\n",
    "        \n",
    "        # Remove headers and footers\n",
    "        text = self.header_footer.sub('', text)\n",
    "        \n",
    "        # Clean up whitespace\n",
    "        text = self.multiple_spaces.sub(' ', text)\n",
    "        \n",
    "        # Clean up line breaks\n",
    "        lines = text.split('\\n')\n",
    "        cleaned_lines = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                cleaned_lines.append(line)\n",
    "        \n",
    "        return '\\n'.join(cleaned_lines)\n",
    "\n",
    "    def clean_file(self, input_file: Path, output_file: Path):\n",
    "        \"\"\"Clean a single chapter file\"\"\"\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        cleaned_text = self.clean_text(text)\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_chapters():\n",
    "    cleaner = TextCleaner()\n",
    "    \n",
    "    # Process each chapter file\n",
    "    for chapter_file in sorted(PROCESSED_DIR.glob(\"chapter_*.txt\")):\n",
    "        print(f\"Cleaning {chapter_file.name}...\")\n",
    "        \n",
    "        # Create cleaned version\n",
    "        cleaned_file = PROCESSED_DIR / f\"cleaned_{chapter_file.name}\"\n",
    "        cleaner.clean_file(chapter_file, cleaned_file)\n",
    "        \n",
    "        # Verify the cleaning\n",
    "        with open(cleaned_file, 'r', encoding='utf-8') as f:\n",
    "            cleaned_text = f.read()\n",
    "            print(f\"  Original size: {chapter_file.stat().st_size}\")\n",
    "            print(f\"  Cleaned size: {cleaned_file.stat().st_size}\")\n",
    "            \n",
    "            # Show a sample of the cleaned text\n",
    "            print(\"\\nSample of cleaned text:\")\n",
    "            print(cleaned_text[:200] + \"...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning chapter_01.txt...\n",
      "  Original size: 76372\n",
      "  Cleaned size: 76372\n",
      "\n",
      "Sample of cleaned text:\n",
      "Chapter 1 Introduction The idea that we learn by interacting with our environment is probably the ﬁrst to occur to us when we think about the nature of learning. When an infant plays, waves its arms, ...\n",
      "\n",
      "Cleaning chapter_02.txt...\n",
      "  Original size: 52647\n",
      "  Cleaned size: 52647\n",
      "\n",
      "Sample of cleaned text:\n",
      "Chapter 2 Multi-armed Bandits The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather...\n",
      "\n",
      "Cleaning chapter_03.txt...\n",
      "  Original size: 73503\n",
      "  Cleaned size: 73503\n",
      "\n",
      "Sample of cleaned text:\n",
      "Chapter 3 Finite Markov Decision Processes In this chapter we introduce the formal problem of ﬁnite Markov decision processes, or ﬁnite MDPs, which we try to solve in the rest of the book. This proble...\n",
      "\n",
      "Cleaning chapter_04.txt...\n",
      "  Original size: 46430\n",
      "  Cleaned size: 46430\n",
      "\n",
      "Sample of cleaned text:\n",
      "Chapter 4 Dynamic Programming The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov d...\n",
      "\n",
      "Cleaning chapter_05.txt...\n",
      "  Original size: 71394\n",
      "  Cleaned size: 71394\n",
      "\n",
      "Sample of cleaned text:\n",
      "Chapter 5 Monte Carlo Methods In this chapter we consider our ﬁrst learning methods for estimating value functions and discovering optimal policies. Unlike the previous chapter, here we do not assume ...\n",
      "\n",
      "Cleaning chapter_06.txt...\n",
      "  Original size: 59391\n",
      "  Cleaned size: 59391\n",
      "\n",
      "Sample of cleaned text:\n",
      "Chapter 6 Temporal-Di↵erence Learning If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be temporal-di↵erence (TD) learning. TD learning is a combina...\n",
      "\n",
      "Cleaning chapter_07.txt...\n",
      "  Original size: 38825\n",
      "  Cleaned size: 38825\n",
      "\n",
      "Sample of cleaned text:\n",
      "Chapter 7 n-step Bootstrapping In this chapter we unify the Monte Carlo (MC) methods and the one-step temporal- di↵erence (TD) methods presented in the previous two chapters. Neither MC methods nor on...\n",
      "\n",
      "Cleaning chapter_08.txt...\n",
      "  Original size: 105147\n",
      "  Cleaned size: 105147\n",
      "\n",
      "Sample of cleaned text:\n",
      "Chapter 8 Planning and Learning with Tabular Methods In this chapter we develop a uniﬁed view of reinforcement learning methods that require a model of the environment, such as dynamic programming and...\n",
      "\n",
      "Cleaning chapter_09.txt...\n",
      "  Original size: 126804\n",
      "  Cleaned size: 126804\n",
      "\n",
      "Sample of cleaned text:\n",
      "Chapter 9 On-policy Prediction with Approximation In this chapter, we begin our study of function approximation in reinforcement learning by considering its use in estimating the state-value function ...\n",
      "\n",
      "Cleaning chapter_10.txt...\n",
      "  Original size: 31204\n",
      "  Cleaned size: 31204\n",
      "\n",
      "Sample of cleaned text:\n",
      "Chapter 10 On-policy Control with Approximation In this chapter we return to the control problem, now with parametric approximation of the action-value function ˆq(s, a,w)⇡q⇤(s, a), where w2Rdis a ﬁni...\n",
      "\n",
      "Cleaning chapter_11.txt...\n",
      "  Original size: 104224\n",
      "  Cleaned size: 104224\n",
      "\n",
      "Sample of cleaned text:\n",
      "Chapter 11 *O↵-policy Methods with Approximation This book has treated on-policy and o↵-policy learning methods since Chapter 5 primarily as two alternative ways of handling the conﬂict between exploi...\n",
      "\n",
      "Cleaning chapter_12.txt...\n",
      "  Original size: 78951\n",
      "  Cleaned size: 78951\n",
      "\n",
      "Sample of cleaned text:\n",
      "Chapter 12 Eligibility Traces Eligibility traces are one of the basic mechanisms of reinforcement learning. For example, in the popular TD( \u0000) algorithm, the \u0000refers to the use of an eligibility trace...\n",
      "\n",
      "Cleaning chapter_13.txt...\n",
      "  Original size: 42720\n",
      "  Cleaned size: 42720\n",
      "\n",
      "Sample of cleaned text:\n",
      "Chapter 13 Policy Gradient Methods In this chapter we consider something new. So far in this book almost all the methods have been action-value methods ; they learned the values of actions and then se...\n",
      "\n",
      "Cleaning chapter_14.txt...\n",
      "  Original size: 116837\n",
      "  Cleaned size: 116837\n",
      "\n",
      "Sample of cleaned text:\n",
      "Chapter 14 Psychology In previous chapters we developed ideas for algorithms based on computational con- siderations alone. In this chapter we look at some of these algorithms from another perspective...\n",
      "\n",
      "Cleaning chapter_15.txt...\n",
      "  Original size: 143311\n",
      "  Cleaned size: 143311\n",
      "\n",
      "Sample of cleaned text:\n",
      "Chapter 15 Neuroscience Neuroscience is the multidisciplinary study of nervous systems: how they regulate bodily functions; control behavior; change over time as a result of development, learning, and...\n",
      "\n",
      "Cleaning chapter_16.txt...\n",
      "  Original size: 139259\n",
      "  Cleaned size: 139259\n",
      "\n",
      "Sample of cleaned text:\n",
      "Chapter 16 Applications and Case Studies In this chapter we present a few case studies of reinforcement learning. Several of these are substantial applications of potential economic signiﬁcance. One, ...\n",
      "\n",
      "Cleaning chapter_17.txt...\n",
      "  Original size: 223336\n",
      "  Cleaned size: 223336\n",
      "\n",
      "Sample of cleaned text:\n",
      "Chapter 17 Frontiers In this ﬁnal chapter we touch on some topics that are beyond the scope of this book but that we see as particularly important for the future of reinforcement learning. Many of the...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    clean_chapters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For section detection, we need to handle:\n",
    "\n",
    "\n",
    "- Full sections (e.g., \"** 1.1 Reinforcement Learning**\")\n",
    "- Referenced sections (e.g., \"Section 17.3\")\n",
    "\n",
    "\n",
    "For NULL markers, we need to:\n",
    "\n",
    "\n",
    " - Replace NULL markers in equations context\n",
    " - Handle the special case where NULL represents \"ffi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Chapter 01\n",
      "Title: Introduction\n",
      "Sections found:\n",
      "  1.1: 10761 characters\n",
      "  1.2: 4220 characters\n",
      "  1.3: 4874 characters\n",
      "  1.4: 3451 characters\n",
      "  1.5: 15546 characters\n",
      "  1.6: 1410 characters\n",
      "  1.7: 33870 characters\n",
      "Processed Chapter 02\n",
      "Title: Multi-armed Bandits\n",
      "Sections found:\n",
      "  2.1: 37829 characters\n",
      "  2.10: 8382 characters\n",
      "  2.2: 2737 characters\n",
      "  2.3: 5535 characters\n",
      "  2.4: 2799 characters\n",
      "  2.5: 4359 characters\n",
      "  2.6: 4639 characters\n",
      "  2.7: 3313 characters\n",
      "  2.8: 1023 characters\n",
      "  2.9: 21126 characters\n",
      "Processed Chapter 03\n",
      "Title: Finite Markov Decision\n",
      "Sections found:\n",
      "  3.1: 11689 characters\n",
      "  3.2: 134 characters\n",
      "  3.3: 5097 characters\n",
      "  3.4: 7672 characters\n",
      "  3.5: 1242 characters\n",
      "  3.6: 280 characters\n",
      "  3.7: 578 characters\n",
      "  3.8: 44244 characters\n",
      "Processed Chapter 04\n",
      "Title: Dynamic Programming\n",
      "Sections found:\n",
      "  4.1: 6026 characters\n",
      "  4.2: 518 characters\n",
      "  4.3: 11471 characters\n",
      "  4.4: 293 characters\n",
      "  4.5: 278 characters\n",
      "  4.6: 336 characters\n",
      "  4.7: 6093 characters\n",
      "  4.8: 18450 characters\n",
      "Processed Chapter 05\n",
      "Title: Monte Carlo Methods\n",
      "Sections found:\n",
      "  5.1: 47762 characters\n",
      "  5.10: 19532 characters\n",
      "  5.2: 8415 characters\n",
      "  5.3: 6041 characters\n",
      "  5.4: 9034 characters\n",
      "  5.5: 15215 characters\n",
      "  5.6: 144 characters\n",
      "  5.7: 290 characters\n",
      "  5.8: 2394 characters\n",
      "  5.9: 19704 characters\n",
      "Processed Chapter 06\n",
      "Title: Temporal-Di↵erence Learning\n",
      "Sections found:\n",
      "  6.1: 10149 characters\n",
      "  6.2: 4818 characters\n",
      "  6.3: 0 characters\n",
      "  6.4: 1758 characters\n",
      "  6.5: 294 characters\n",
      "  6.6: 8144 characters\n",
      "  6.7: 2840 characters\n",
      "  6.8: 21458 characters\n",
      "  6.9: 7936 characters\n",
      "Processed Chapter 07\n",
      "Title: n-step Bootstrapping\n",
      "Sections found:\n",
      "  7.1: 4617 characters\n",
      "  7.2: 3926 characters\n",
      "  7.3: 3875 characters\n",
      "  7.4: 7554 characters\n",
      "  7.5: 1224 characters\n",
      "  7.6: 115 characters\n",
      "  7.7: 14751 characters\n",
      "Processed Chapter 08\n",
      "Title: Planning and Learning with\n",
      "Sections found:\n",
      "  8.1: 69666 characters\n",
      "  8.10: 6197 characters\n",
      "  8.11: 8232 characters\n",
      "  8.12: 3599 characters\n",
      "  8.13: 7567 characters\n",
      "  8.2: 9489 characters\n",
      "  8.3: 7556 characters\n",
      "  8.4: 432 characters\n",
      "  8.5: 17112 characters\n",
      "  8.6: 1360 characters\n",
      "  8.7: 9807 characters\n",
      "  8.8: 15070 characters\n",
      "  8.9: 39334 characters\n",
      "Processed Chapter 09\n",
      "Title: On-policy Prediction with\n",
      "Sections found:\n",
      "  9.1: 52151 characters\n",
      "  9.10: 4537 characters\n",
      "  9.11: 1660 characters\n",
      "  9.12: 65161 characters\n",
      "  9.2: 5050 characters\n",
      "  9.3: 11437 characters\n",
      "  9.4: 12085 characters\n",
      "  9.5: 28031 characters\n",
      "  9.6: 7340 characters\n",
      "  9.7: 21875 characters\n",
      "  9.8: 37920 characters\n",
      "  9.9: 36387 characters\n",
      "Processed Chapter 10\n",
      "Title: On-policy Control with\n",
      "Sections found:\n",
      "  10.1: 5791 characters\n",
      "  10.2: 2454 characters\n",
      "  10.3: 141 characters\n",
      "  10.4: 5776 characters\n",
      "  10.5: 670 characters\n",
      "  10.6: 14388 characters\n",
      "Processed Chapter 11\n",
      "Title: *O↵-policy Methods with\n",
      "Sections found:\n",
      "  11.1: 93101 characters\n",
      "  11.10: 3820 characters\n",
      "  11.2: 10956 characters\n",
      "  11.3: 5934 characters\n",
      "  11.4: 16858 characters\n",
      "  11.5: 12977 characters\n",
      "  11.6: 25939 characters\n",
      "  11.7: 47239 characters\n",
      "  11.8: 4525 characters\n",
      "  11.9: 10068 characters\n",
      "Processed Chapter 12\n",
      "Title: Eligibility Traces\n",
      "Sections found:\n",
      "  12.1: 37496 characters\n",
      "  12.10: 14127 characters\n",
      "  12.11: 1068 characters\n",
      "  12.12: 2638 characters\n",
      "  12.13: 15106 characters\n",
      "  12.2: 2932 characters\n",
      "  12.3: 10814 characters\n",
      "  12.4: 3017 characters\n",
      "  12.5: 9619 characters\n",
      "  12.6: 4012 characters\n",
      "  12.7: 10103 characters\n",
      "  12.8: 19728 characters\n",
      "  12.9: 14825 characters\n",
      "  9.11: 12193 characters\n",
      "Processed Chapter 13\n",
      "Title: Policy Gradient Methods\n",
      "Sections found:\n",
      "  13.1: 6099 characters\n",
      "  13.2: 4850 characters\n",
      "  13.3: 6887 characters\n",
      "  13.4: 3622 characters\n",
      "  13.5: 4157 characters\n",
      "  13.6: 3358 characters\n",
      "  13.7: 3910 characters\n",
      "  13.8: 6179 characters\n",
      "Processed Chapter 14\n",
      "Title: Psychology\n",
      "Sections found:\n",
      "  14.1: 3985 characters\n",
      "  14.2: 35119 characters\n",
      "  14.3: 3993 characters\n",
      "  14.4: 24666 characters\n",
      "  14.5: 4796 characters\n",
      "  14.6: 13979 characters\n",
      "  14.7: 25805 characters\n",
      "Processed Chapter 15\n",
      "Title: Neuroscience\n",
      "Sections found:\n",
      "  15.1: 25935 characters\n",
      "  15.10: 71055 characters\n",
      "  15.11: 6140 characters\n",
      "  15.12: 3345 characters\n",
      "  15.13: 10851 characters\n",
      "  15.2: 6322 characters\n",
      "  15.3: 1004 characters\n",
      "  15.4: 17016 characters\n",
      "  15.5: 8651 characters\n",
      "  15.6: 42350 characters\n",
      "  15.7: 9753 characters\n",
      "  15.8: 14310 characters\n",
      "  15.9: 59233 characters\n",
      "Processed Chapter 16\n",
      "Title: Applications and Case Studies\n",
      "Sections found:\n",
      "  16.1: 20130 characters\n",
      "  16.2: 8546 characters\n",
      "  16.3: 10296 characters\n",
      "  16.4: 14054 characters\n",
      "  16.5: 19271 characters\n",
      "  16.6: 23369 characters\n",
      "  16.7: 19222 characters\n",
      "  16.8: 17554 characters\n",
      "  2.5: 4558 characters\n",
      "Processed Chapter 17\n",
      "Title: Frontiers\n",
      "Sections found:\n",
      "  17.1: 6790 characters\n",
      "  17.2: 8800 characters\n",
      "  17.3: 14919 characters\n",
      "  17.4: 11943 characters\n",
      "  17.5: 10307 characters\n",
      "  17.6: 167682 characters\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "class RLTextProcessor:\n",
    "    def __init__(self):\n",
    "        self.replacements = {\n",
    "            'NUL': 'ffi',\n",
    "            '↵': 'ff',\n",
    "            '  ': ' ',\n",
    "            'ﬁ': 'fi',\n",
    "            'ﬂ': 'fl'\n",
    "        }\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Basic text cleaning with specific replacements\"\"\"\n",
    "        cleaned = text\n",
    "        for old, new in self.replacements.items():\n",
    "            cleaned = cleaned.replace(old, new)\n",
    "        return re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "\n",
    "    def find_section_content(self, text: str, section_num: str, next_section_num: str = None) -> str:\n",
    "        \"\"\"Find content between current section and next section\"\"\"\n",
    "        # Create pattern for current section\n",
    "        section_pattern = f\"{section_num}\\\\s+\"\n",
    "        section_start = re.search(section_pattern, text)\n",
    "        \n",
    "        if not section_start:\n",
    "            return \"\"\n",
    "            \n",
    "        start_pos = section_start.end()\n",
    "        \n",
    "        # If we have a next section, find its position\n",
    "        if next_section_num:\n",
    "            next_pattern = f\"{next_section_num}\\\\s+\"\n",
    "            next_match = re.search(next_pattern, text[start_pos:])\n",
    "            if next_match:\n",
    "                end_pos = start_pos + next_match.start()\n",
    "            else:\n",
    "                end_pos = len(text)\n",
    "        else:\n",
    "            end_pos = len(text)\n",
    "            \n",
    "        content = text[start_pos:end_pos].strip()\n",
    "        return self.clean_text(content)\n",
    "\n",
    "    def process_chapter(self, text: str, metadata: Dict) -> Dict:\n",
    "        \"\"\"Process chapter using metadata sections\"\"\"\n",
    "        # Get ordered list of sections\n",
    "        sections = metadata[\"sections\"]\n",
    "        section_nums = sorted(sections.keys())\n",
    "        \n",
    "        processed_sections = {}\n",
    "        \n",
    "        # Process each section\n",
    "        for i, section_num in enumerate(section_nums):\n",
    "            next_section = section_nums[i + 1] if i < len(section_nums) - 1 else None\n",
    "            content = self.find_section_content(text, section_num, next_section)\n",
    "            processed_sections[section_num] = content\n",
    "            \n",
    "        return {\n",
    "            \"title\": metadata[\"title\"],\n",
    "            \"sections\": processed_sections\n",
    "        }\n",
    "\n",
    "def process_all_chapters(base_dir: str = \"./\") -> None:\n",
    "    \"\"\"Process all chapters using metadata files\"\"\"\n",
    "    processor = RLTextProcessor()\n",
    "    \n",
    "    # Process chapters 1 through 17\n",
    "    for chapter_num in range(1, 18):\n",
    "        chapter_num_str = f\"{chapter_num:02d}\"\n",
    "        try:\n",
    "            # Read metadata\n",
    "            meta_path = Path(base_dir) / f\"chapter_{chapter_num_str}_meta.json\"\n",
    "            with open(meta_path, 'r', encoding='utf-8') as f:\n",
    "                metadata = json.load(f)\n",
    "                \n",
    "            # Read chapter text\n",
    "            text_path = Path(base_dir) / f\"chapter_{chapter_num_str}.txt\"\n",
    "            with open(text_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                \n",
    "            # Process chapter\n",
    "            result = processor.process_chapter(text, metadata)\n",
    "            \n",
    "            # Save processed content\n",
    "            output_path = Path(base_dir) / f\"chapter_{chapter_num_str}_processed.json\"\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "                \n",
    "            print(f\"Processed Chapter {chapter_num_str}\")\n",
    "            # Debug output\n",
    "            print(f\"Title: {result['title']}\")\n",
    "            print(\"Sections found:\")\n",
    "            for section_num, content in result['sections'].items():\n",
    "                print(f\"  {section_num}: {len(content)} characters\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing Chapter {chapter_num_str}: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_all_chapters(\"data/processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Chapter 01\n",
      "Original length: 75562 characters\n",
      "Cleaned length: 75762 characters\n",
      "Processed Chapter 02\n",
      "Original length: 52092 characters\n",
      "Cleaned length: 52067 characters\n",
      "Processed Chapter 03\n",
      "Original length: 72439 characters\n",
      "Cleaned length: 72390 characters\n",
      "Processed Chapter 04\n",
      "Original length: 45724 characters\n",
      "Cleaned length: 45650 characters\n",
      "Processed Chapter 05\n",
      "Original length: 70263 characters\n",
      "Cleaned length: 70160 characters\n",
      "Processed Chapter 06\n",
      "Original length: 58676 characters\n",
      "Cleaned length: 58687 characters\n",
      "Processed Chapter 07\n",
      "Original length: 38040 characters\n",
      "Cleaned length: 37856 characters\n",
      "Processed Chapter 08\n",
      "Original length: 104392 characters\n",
      "Cleaned length: 104501 characters\n",
      "Processed Chapter 09\n",
      "Original length: 125415 characters\n",
      "Cleaned length: 125447 characters\n",
      "Processed Chapter 10\n",
      "Original length: 30514 characters\n",
      "Cleaned length: 30433 characters\n",
      "Processed Chapter 11\n",
      "Original length: 102574 characters\n",
      "Cleaned length: 102328 characters\n",
      "Processed Chapter 12\n",
      "Original length: 41370 characters\n",
      "Cleaned length: 41308 characters\n",
      "Processed Chapter 13\n",
      "Original length: 115673 characters\n",
      "Cleaned length: 115824 characters\n",
      "Processed Chapter 14\n",
      "Original length: 141932 characters\n",
      "Cleaned length: 141934 characters\n",
      "Processed Chapter 15\n",
      "Original length: 137722 characters\n",
      "Cleaned length: 137820 characters\n",
      "Processed Chapter 16\n",
      "Original length: 220341 characters\n",
      "Cleaned length: 219806 characters\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "class TextCleaner:\n",
    "    def __init__(self):\n",
    "        # Unicode replacements\n",
    "        self.unicode_map = {\n",
    "            '\\ufb01': 'fi',  # fi ligature\n",
    "            '\\ufb02': 'fl',  # fl ligature\n",
    "            '\\u21b5': 'ff',  # ↵ to ff\n",
    "            '\\u0000': '',    # null character\n",
    "            '\\u21b5': 'ff',  # another variant of ↵\n",
    "            '\\u0003': '',    # end of text character\n",
    "            '\\u0002': '',    # start of text character\n",
    "            '\\u0001': '',    # start of heading\n",
    "            '\\u0004': '',    # end of transmission\n",
    "            '\\u2019': \"'\",   # right single quotation mark\n",
    "            '\\u201c': '\"',   # left double quotation mark\n",
    "            '\\u201d': '\"',   # right double quotation mark\n",
    "            '\\u2013': '-',   # en dash\n",
    "            '\\u2014': '--',  # em dash\n",
    "            '\\u00ac': '-',   # not sign\n",
    "            '\\u2022': '*',   # bullet\n",
    "            '\\u00b5': 'μ',   # micro sign\n",
    "            '\\u03c0': 'π',   # pi\n",
    "            '\\u03b5': 'ε',   # epsilon\n",
    "            '\\u03b1': 'α',   # alpha\n",
    "            '\\u03b2': 'β',   # beta\n",
    "            '\\u03b3': 'γ',   # gamma\n",
    "            '\\u03b4': 'δ',   # delta\n",
    "            '\\u03bb': 'λ',   # lambda\n",
    "            '\\u03c3': 'σ',   # sigma\n",
    "            '\\u03c4': 'τ',   # tau\n",
    "            '\\u03c9': 'ω',   # omega\n",
    "            '\\u2192': '->',  # rightward arrow\n",
    "            '\\u2190': '<-',  # leftward arrow\n",
    "            '\\u2194': '<->',  # bidirectional arrow\n",
    "            '\\u21d2': '=>',  # rightward double arrow\n",
    "            '\\u21d0': '<=',  # leftward double arrow\n",
    "            '\\u21d4': '<=>',  # bidirectional double arrow\n",
    "            '\\u2208': 'in',  # element of\n",
    "            '\\u2209': 'not in',  # not an element of\n",
    "            '\\u220b': 'ni',  # contains as member\n",
    "            '\\u2229': 'intersection',  # intersection\n",
    "            '\\u222a': 'union',  # union\n",
    "            '\\u2264': '<=',  # less-than or equal to\n",
    "            '\\u2265': '>=',  # greater-than or equal to\n",
    "            '\\u221e': 'infinity',  # infinity\n",
    "            '\\u2248': '≈',  # almost equal to\n",
    "            '\\u2260': '!=',  # not equal to\n",
    "            '\\u00d7': 'x',  # multiplication sign\n",
    "            '\\u2217': '*',  # asterisk operator\n",
    "            '\\u221a': 'sqrt',  # square root\n",
    "            '\\u223c': '~',  # tilde operator\n",
    "            '\\u2026': '...',  # horizontal ellipsis\n",
    "            '\\u00a0': ' ',  # non-breaking space\n",
    "            '\\u02c6': '^',  # circumflex\n",
    "            '\\u02dc': '~',  # small tilde\n",
    "            '\\u02d8': 'u',  # breve\n",
    "            '\\u02d9': '.',  # dot above\n",
    "            '\\u02da': 'o',  # ring above\n",
    "            '\\u02db': ',',  # ogonek\n",
    "            '\\u02dc': '~',  # small tilde\n",
    "            '\\u02dd': '\"',  # double acute accent\n",
    "        }\n",
    "        \n",
    "        # Common text patterns to fix\n",
    "        self.text_replacements = {\n",
    "            'NUL': 'ffi',  # Common NUL replacement\n",
    "            '  ': ' ',     # Double spaces\n",
    "            ' ,': ',',     # Space before comma\n",
    "            ' .': '.',     # Space before period\n",
    "            '( ': '(',     # Space after opening parenthesis\n",
    "            ' )': ')',     # Space before closing parenthesis\n",
    "            '\\n\\n\\n': '\\n\\n',  # Multiple newlines\n",
    "            '--': '-',     # Double hyphens\n",
    "        }\n",
    "\n",
    "    def clean_unicode(self, text: str) -> str:\n",
    "        \"\"\"Replace Unicode characters with their ASCII equivalents\"\"\"\n",
    "        cleaned = text\n",
    "        for unicode_char, replacement in self.unicode_map.items():\n",
    "            cleaned = cleaned.replace(unicode_char, replacement)\n",
    "        return cleaned\n",
    "\n",
    "    def clean_text_patterns(self, text: str) -> str:\n",
    "        \"\"\"Clean common text patterns\"\"\"\n",
    "        cleaned = text\n",
    "        for pattern, replacement in self.text_replacements.items():\n",
    "            cleaned = cleaned.replace(pattern, replacement)\n",
    "        # Remove excessive whitespace\n",
    "        cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "        return cleaned\n",
    "\n",
    "    def clean(self, text: str) -> str:\n",
    "        \"\"\"Full text cleaning pipeline\"\"\"\n",
    "        cleaned = self.clean_unicode(text)\n",
    "        cleaned = self.clean_text_patterns(cleaned)\n",
    "        return cleaned\n",
    "\n",
    "def process_raw_chapters(base_dir: str = \"./\") -> None:\n",
    "    \"\"\"Extract and clean chapter content\"\"\"\n",
    "    cleaner = TextCleaner()\n",
    "    base_path = Path(base_dir)\n",
    "    raw_output_dir = base_path / \"raw_json\"\n",
    "    raw_output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    for chapter_num in range(1, 17):\n",
    "        chapter_num_str = f\"{chapter_num:02d}\"\n",
    "        try:\n",
    "            # Read chapter text\n",
    "            input_path = base_path / f\"chapter_{chapter_num_str}.txt\"\n",
    "            with open(input_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            # Clean the text\n",
    "            cleaned_text = cleaner.clean(text)\n",
    "            \n",
    "            # Create structure\n",
    "            raw_chapter = {\n",
    "                \"chapter\": chapter_num_str,\n",
    "                \"content\": cleaned_text\n",
    "            }\n",
    "            \n",
    "            # Save to JSON\n",
    "            output_path = raw_output_dir / f\"chapter_{chapter_num_str}_raw.json\"\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(raw_chapter, f, indent=2, ensure_ascii=False)\n",
    "                \n",
    "            print(f\"Processed Chapter {chapter_num_str}\")\n",
    "            print(f\"Original length: {len(text)} characters\")\n",
    "            print(f\"Cleaned length: {len(cleaned_text)} characters\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing Chapter {chapter_num_str}: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_raw_chapters(\"data/processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Chapter 01: Introduction\n",
      "Found 7 sections\n",
      "Processed Chapter 02: Multi-armed Bandits\n",
      "Found 9 sections\n",
      "Processed Chapter 03: Finite Markov Decision Processes\n",
      "Found 6 sections\n",
      "Processed Chapter 04: Dynamic Programming\n",
      "Found 7 sections\n",
      "Processed Chapter 05: Monte Carlo Methods\n",
      "Found 7 sections\n",
      "Processed Chapter 06: Temporal-Difference Learning\n",
      "Found 8 sections\n",
      "Processed Chapter 07: n-step Bootstrapping\n",
      "Found 5 sections\n",
      "Processed Chapter 08: Planning and Learning with Tabular Methods\n",
      "Found 13 sections\n",
      "Processed Chapter 09: On-policy Prediction with Approximation\n",
      "Found 11 sections\n",
      "Processed Chapter 10: On-policy Control with Approximation\n",
      "Found 5 sections\n",
      "Processed Chapter 11: *Off-policy Methods with Approximation\n",
      "Found 9 sections\n",
      "Processed Chapter 12: Policy Gradient Methods\n",
      "Found 7 sections\n",
      "Processed Chapter 13: Psychology\n",
      "Found 7 sections\n",
      "Processed Chapter 14: Neuroscience\n",
      "Found 12 sections\n",
      "Processed Chapter 15: Applications and Case Studies\n",
      "Found 6 sections\n",
      "Processed Chapter 16: Frontiers\n",
      "Found 5 sections\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "def find_section_boundaries(content: str, sections: Dict[str, str]) -> Dict[str, Tuple[int, int]]:\n",
    "    boundaries = {}\n",
    "    ordered_sections = sorted(sections.keys())\n",
    "    \n",
    "    for i, section_num in enumerate(ordered_sections):\n",
    "        section_title = sections[section_num]\n",
    "        # Escape special characters in title and use raw string for pattern\n",
    "        safe_title = re.escape(section_title)\n",
    "        pattern = rf\"{re.escape(section_num)}\\s*{safe_title}\"\n",
    "        match = re.search(pattern, content)\n",
    "        \n",
    "        if match:\n",
    "            start_pos = match.start()\n",
    "            end_pos = len(content)\n",
    "            \n",
    "            if i < len(ordered_sections) - 1:\n",
    "                next_section = ordered_sections[i + 1]\n",
    "                next_title = sections[next_section]\n",
    "                next_pattern = rf\"{re.escape(next_section)}\\s*{re.escape(next_title)}\"\n",
    "                next_match = re.search(next_pattern, content)\n",
    "                if next_match:\n",
    "                    end_pos = next_match.start()\n",
    "            \n",
    "            boundaries[section_num] = (start_pos, end_pos)\n",
    "    \n",
    "    return boundaries\n",
    "\n",
    "def process_sections(base_dir: str = \"./\") -> None:\n",
    "    base_path = Path(base_dir)\n",
    "    output_dir = base_path / \"final_json\"\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    for chapter_num in range(1, 17):\n",
    "        chapter_num_str = f\"{chapter_num:02d}\"\n",
    "        try:\n",
    "            chapter_file = base_path / f\"chapter_{chapter_num_str}.txt\"\n",
    "            with open(chapter_file, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            meta_file = base_path / f\"chapter_{chapter_num_str}_meta.json\"\n",
    "            with open(meta_file, 'r', encoding='utf-8') as f:\n",
    "                metadata = json.load(f)\n",
    "            \n",
    "            section_boundaries = find_section_boundaries(content, metadata[\"sections\"])\n",
    "            \n",
    "            sections = {}\n",
    "            for section_num, (start, end) in section_boundaries.items():\n",
    "                sections[section_num] = content[start:end].strip()\n",
    "            \n",
    "            chapter_data = {\n",
    "                \"title\": metadata[\"title\"],\n",
    "                \"sections\": sections\n",
    "            }\n",
    "            \n",
    "            output_file = output_dir / f\"chapter_{chapter_num_str}_sections.json\"\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(chapter_data, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"Processed Chapter {chapter_num_str}: {metadata['title']}\")\n",
    "            print(f\"Found {len(sections)} sections\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing Chapter {chapter_num_str}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_sections(\"data/processed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
