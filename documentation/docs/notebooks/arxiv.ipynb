{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from typing import Dict, List, Optional\n",
    "import logging\n",
    "from urllib.parse import urlparse\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivDownloader:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the ArXiv downloader with fixed paths\"\"\"\n",
    "        # Set fixed paths\n",
    "        self.base_dir = Path(\"path to the project\")\n",
    "        self.base_path = self.base_dir / \"data\"\n",
    "        self.pdfs_dir = self.base_path / \"pdfs\"\n",
    "        self.metadata_dir = self.base_path / \"metadata\"\n",
    "        \n",
    "        self._setup_directories()\n",
    "        self._setup_logging()\n",
    "        \n",
    "    def _setup_directories(self):\n",
    "        \"\"\"Create necessary directory structure\"\"\"\n",
    "        self.pdfs_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.metadata_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        current_year = datetime.now().year\n",
    "        for year in range(2017, current_year + 1):\n",
    "            (self.pdfs_dir / str(year)).mkdir(exist_ok=True)\n",
    "            (self.metadata_dir / str(year)).mkdir(exist_ok=True)\n",
    "    \n",
    "    def _setup_logging(self):\n",
    "        \"\"\"Setup logging configuration\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(self.base_path / \"download_log.txt\"),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def _safe_download_pdf(self, paper, pdf_path: Path) -> bool:\n",
    "        \"\"\"Safely download PDF with proper path handling\"\"\"\n",
    "        try:\n",
    "            # Create directory if it doesn't exist\n",
    "            pdf_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Get PDF URL\n",
    "            pdf_url = next(str(link) for link in paper.links if 'pdf' in str(link))\n",
    "            \n",
    "            # Download directly to the correct path\n",
    "            response = requests.get(pdf_url, allow_redirects=True)\n",
    "            if response.status_code == 200:\n",
    "                with open(pdf_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                return True\n",
    "                \n",
    "            self.logger.error(f\"Failed to download PDF, status code: {response.status_code}\")\n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error downloading PDF: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def download_papers(self, year: int):\n",
    "        \"\"\"Download exactly 100 papers for the specified year\"\"\"\n",
    "        self.logger.info(f\"\\nStarting download for year {year}\")\n",
    "        \n",
    "        # Construct search client\n",
    "        client = arxiv.Client(\n",
    "            page_size=100,\n",
    "            delay_seconds=3.0,\n",
    "            num_retries=5\n",
    "        )\n",
    "        \n",
    "        # Create search with max_results=100\n",
    "        search = arxiv.Search(\n",
    "            query=self._construct_query(year),\n",
    "            max_results=100  # Limit to 100 papers\n",
    "        )\n",
    "        \n",
    "        # Initialize metadata collection\n",
    "        year_metadata = []\n",
    "        papers_processed = 0\n",
    "        \n",
    "        try:\n",
    "            # Collect exactly 100 papers\n",
    "            papers = list(client.results(search))\n",
    "            total_papers = len(papers)\n",
    "            print(f\"\\nFound {total_papers} papers for {year}\")\n",
    "            \n",
    "            if not papers:\n",
    "                self.logger.warning(f\"No papers found for year {year}\")\n",
    "                return 0\n",
    "            \n",
    "            # Process and download papers\n",
    "            print(f\"\\nDownloading PDFs for year {year}:\")\n",
    "            for idx, paper in enumerate(papers, 1):\n",
    "                paper_id = paper.entry_id.split('/')[-1].split('v')[0]  # Remove version number\n",
    "                \n",
    "                try:\n",
    "                    # Extract and save metadata\n",
    "                    metadata = self._extract_metadata(paper)\n",
    "                    year_metadata.append(metadata)\n",
    "                    \n",
    "                    # Download PDF\n",
    "                    pdf_path = self.pdfs_dir / str(year) / f\"{paper_id}.pdf\"\n",
    "                    if not pdf_path.exists():\n",
    "                        if self._safe_download_pdf(paper, pdf_path):\n",
    "                            print(f\"[{idx}/100] Downloaded: {paper.title[:50]}...\")\n",
    "                        else:\n",
    "                            print(f\"[{idx}/100] Failed to download: {paper.title[:50]}...\")\n",
    "                    else:\n",
    "                        print(f\"[{idx}/100] Already exists: {paper.title[:50]}...\")\n",
    "                    \n",
    "                    time.sleep(1)  # Be nice to arXiv servers\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error processing paper {paper_id}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "            # Save metadata for the year\n",
    "            metadata_path = self.metadata_dir / str(year) / \"metadata.json\"\n",
    "            with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(year_metadata, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"\\nYear {year} complete:\")\n",
    "            print(f\"- PDFs downloaded: {len(year_metadata)}\")\n",
    "            print(f\"- Metadata saved to: {metadata_path}\")\n",
    "            \n",
    "            return len(year_metadata)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error downloading papers for year {year}: {str(e)}\")\n",
    "            return 0\n",
    "\n",
    "    def _construct_query(self, year: int) -> str:\n",
    "        \"\"\"Construct arXiv query for RL papers from specific year\"\"\"\n",
    "        return (f'(cat:cs.AI OR cat:cs.LG) AND '\n",
    "                f'(abs:\"reinforcement learning\" OR abs:\"deep reinforcement learning\") AND '\n",
    "                f'submittedDate:[{year}0101 TO {year}1231]')\n",
    "\n",
    "    def _extract_metadata(self, paper) -> Dict:\n",
    "        \"\"\"Extract relevant metadata from arXiv paper\"\"\"\n",
    "        return {\n",
    "            'title': paper.title,\n",
    "            'authors': [str(author) for author in paper.authors],\n",
    "            'abstract': paper.summary,\n",
    "            'categories': paper.categories,\n",
    "            'published': paper.published.strftime('%Y-%m-%d'),\n",
    "            'updated': paper.updated.strftime('%Y-%m-%d'),\n",
    "            'arxiv_id': paper.entry_id.split('/')[-1],\n",
    "            'primary_category': paper.primary_category,\n",
    "            'doi': paper.doi,\n",
    "            'links': [str(link) for link in paper.links]\n",
    "        }\n",
    "\n",
    "    def download_all_years(self, start_year: int = 2024):  # Changed default start_year\n",
    "        \"\"\"Download 100 papers for each year from specified start year to current\"\"\"\n",
    "        current_year = datetime.now().year\n",
    "        total_papers = 0\n",
    "        \n",
    "        for year in range(start_year, current_year + 1):\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Processing year {year}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            papers_downloaded = self.download_papers(year)\n",
    "            total_papers += papers_downloaded\n",
    "            \n",
    "            print(f\"\\nTotal papers downloaded so far: {total_papers}\")\n",
    "            time.sleep(5)  # Wait between years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloader = ArxivDownloader()\n",
    "downloader.download_all_years(start_year=2017)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
