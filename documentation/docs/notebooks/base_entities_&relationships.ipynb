{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "from typing import Dict, List, Set\n",
    "from pathlib import Path\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLEntityExtractor:\n",
    "    def __init__(self, api_key: str = None):\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "            api_key=api_key or \"paste your api key here\"\n",
    "        )\n",
    "        # Track entities across chapters for cross-referencing\n",
    "        self.entity_appearances = defaultdict(set)\n",
    "        self.domain_connections = defaultdict(set)\n",
    "\n",
    "    def create_extract_prompt(self, section_text: str, chapter: str, section: str) -> str:\n",
    "        return f\"\"\"Extract key RL entities and their relationships from this text section. Focus on core concepts, domains, and clear relationships. Format as JSON:\n",
    "\n",
    "{{\n",
    "    \"entities\": [\n",
    "        {{\n",
    "            \"id\": \"unique_snake_case_id\",\n",
    "            \"name\": \"Full Concept Name\",\n",
    "            \"type\": \"concept|algorithm|method|principle|domain\",\n",
    "            \"definition\": \"Clear, precise definition under 50 words\",\n",
    "            \"domains\": [\"psychology\", \"neuroscience\", \"mathematics\", etc],\n",
    "            \"properties\": [\n",
    "                {{\n",
    "                    \"name\": \"property_name\",\n",
    "                    \"value\": \"property_value\",\n",
    "                    \"type\": \"characteristic|parameter|constraint|requirement\"\n",
    "                }}\n",
    "            ],\n",
    "            \"source\": {{\n",
    "                \"chapter\": \"{chapter}\",\n",
    "                \"section\": \"{section}\",\n",
    "                \"context\": \"Brief context where this appears\"\n",
    "            }}\n",
    "        }}\n",
    "    ],\n",
    "    \"relationships\": [\n",
    "        {{\n",
    "            \"source_id\": \"entity_id\",\n",
    "            \"target_id\": \"entity_id\",\n",
    "            \"type\": \"is_part_of|implements|uses|relates_to|influences\",\n",
    "            \"description\": \"Brief description of relationship\"\n",
    "        }}\n",
    "    ],\n",
    "    \"domains_discussed\": [\"list\", \"of\", \"domains\"],\n",
    "    \"key_equations\": [\n",
    "        {{\n",
    "            \"id\": \"equation_id\",\n",
    "            \"name\": \"Equation Name\",\n",
    "            \"latex\": \"LaTeX representation\",\n",
    "            \"description\": \"What this equation represents\"\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Extract ONLY well-defined, clearly stated concepts. Skip ambiguous or unclear references.\n",
    "Focus on reinforcement learning concepts and their connections to other domains.\n",
    "\n",
    "Text to analyze:\n",
    "{section_text}\"\"\"\n",
    "\n",
    "    def clean_json_response(self, response_text: str) -> dict:\n",
    "        \"\"\"Clean and parse JSON from API response.\"\"\"\n",
    "        try:\n",
    "            # Find JSON content between markers if present\n",
    "            import re\n",
    "            json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_str = json_match.group(0)\n",
    "                return json.loads(json_str)\n",
    "            return {}\n",
    "        except Exception as e:\n",
    "            print(f\"Error cleaning JSON: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def update_cross_references(self, entities: List[Dict], chapter: str) -> None:\n",
    "        \"\"\"Update entity appearance tracking.\"\"\"\n",
    "        for entity in entities:\n",
    "            entity_id = entity['id']\n",
    "            self.entity_appearances[entity_id].add(chapter)\n",
    "            \n",
    "            # Track domain connections\n",
    "            if 'domains' in entity:\n",
    "                for domain in entity['domains']:\n",
    "                    self.domain_connections[domain].add(entity_id)\n",
    "\n",
    "    def process_section(self, section_text: str, chapter: str, section: str) -> Dict:\n",
    "        \"\"\"Process a single section and extract entities.\"\"\"\n",
    "        try:\n",
    "            completion = self.client.chat.completions.create(\n",
    "                model=\"nvidia/llama-3.1-nemotron-70b-instruct\",\n",
    "                messages=[{\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": self.create_extract_prompt(section_text, chapter, section)\n",
    "                }],\n",
    "                temperature=0.3,\n",
    "                max_tokens=2048\n",
    "            )\n",
    "            \n",
    "            if completion.choices:\n",
    "                response_text = completion.choices[0].message.content\n",
    "                extracted = self.clean_json_response(response_text)\n",
    "                \n",
    "                if 'entities' in extracted:\n",
    "                    self.update_cross_references(extracted['entities'], chapter)\n",
    "                \n",
    "                return extracted\n",
    "            \n",
    "            return {}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing section: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def process_chapter_file(self, file_path: Path) -> Dict:\n",
    "        \"\"\"Process a single chapter file.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                sections = json.load(f)\n",
    "            \n",
    "            chapter_data = {\n",
    "                'chapter_id': file_path.stem,\n",
    "                'entities': [],\n",
    "                'relationships': [],\n",
    "                'domains': set(),\n",
    "                'equations': []\n",
    "            }\n",
    "            \n",
    "            for section_id, content in sections.items():\n",
    "                print(f\"Processing {file_path.stem} - {section_id}\")\n",
    "                section_data = self.process_section(\n",
    "                    content, \n",
    "                    chapter=file_path.stem, \n",
    "                    section=section_id\n",
    "                )\n",
    "                \n",
    "                if section_data:\n",
    "                    chapter_data['entities'].extend(section_data.get('entities', []))\n",
    "                    chapter_data['relationships'].extend(section_data.get('relationships', []))\n",
    "                    chapter_data['domains'].update(section_data.get('domains_discussed', []))\n",
    "                    chapter_data['equations'].extend(section_data.get('key_equations', []))\n",
    "            \n",
    "            # Convert sets to lists for JSON serialization\n",
    "            chapter_data['domains'] = list(chapter_data['domains'])\n",
    "            \n",
    "            return chapter_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chapter file {file_path}: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def generate_knowledge_graph(self, input_dir: str, output_dir: str):\n",
    "        \"\"\"Generate complete knowledge graph from all chapters.\"\"\"\n",
    "        input_path = Path(input_dir)\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Process all chapter files\n",
    "        all_chapters_data = []\n",
    "        for file in sorted(input_path.glob(\"chapter_*.json\")):\n",
    "            chapter_data = self.process_chapter_file(file)\n",
    "            if chapter_data:\n",
    "                all_chapters_data.append(chapter_data)\n",
    "\n",
    "        # Create final knowledge graph with cross-references\n",
    "        knowledge_graph = {\n",
    "            'entities': {},\n",
    "            'relationships': [],\n",
    "            'domain_hierarchy': {},\n",
    "            'cross_references': dict(self.entity_appearances),\n",
    "            'domain_connections': dict(self.domain_connections),\n",
    "            'metadata': {\n",
    "                'total_chapters': len(all_chapters_data),\n",
    "                'total_entities': sum(len(c['entities']) for c in all_chapters_data),\n",
    "                'total_relationships': sum(len(c['relationships']) for c in all_chapters_data)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Merge entities and relationships from all chapters\n",
    "        for chapter in all_chapters_data:\n",
    "            for entity in chapter['entities']:\n",
    "                entity_id = entity['id']\n",
    "                if entity_id not in knowledge_graph['entities']:\n",
    "                    knowledge_graph['entities'][entity_id] = entity\n",
    "                else:\n",
    "                    # Merge appearances and properties\n",
    "                    existing = knowledge_graph['entities'][entity_id]\n",
    "                    existing['source'] = [existing['source']] if isinstance(existing['source'], dict) else existing['source']\n",
    "                    existing['source'].append(entity['source'])\n",
    "\n",
    "            knowledge_graph['relationships'].extend(chapter['relationships'])\n",
    "\n",
    "        # Save the complete knowledge graph\n",
    "        output_file = output_path / \"rl_knowledge_graph.json\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(knowledge_graph, f, indent=2)\n",
    "\n",
    "        return knowledge_graph\n",
    "\n",
    "def main():\n",
    "    extractor = RLEntityExtractor()\n",
    "    input_dir = r\"path to folder containing json files for chapters of the book we processed in the previous step \"\n",
    "    output_dir = r\"path to output directory\"\n",
    "    \n",
    "    try:\n",
    "        knowledge_graph = extractor.generate_knowledge_graph(input_dir, output_dir)\n",
    "        print(f\"\\nKnowledge Graph Generation Complete:\")\n",
    "        print(f\"Total Entities: {knowledge_graph['metadata']['total_entities']}\")\n",
    "        print(f\"Total Relationships: {knowledge_graph['metadata']['total_relationships']}\")\n",
    "        print(f\"Output saved to: {output_dir}/rl_knowledge_graph.json\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during knowledge graph generation: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing entity 1/64: reinforcement_learning\n",
      "Processing entity 2/64: markov_decision_process\n",
      "Error cleaning JSON: Expecting ',' delimiter: line 14 column 13 (char 520)\n",
      "Problematic text: {\n",
      "    \"relationships\": [\n",
      "        {\n",
      "            \"source\": \"markov_decision_process\",\n",
      "            \"source_layer\": \"foundation_layer\",\n",
      "            \"target\": \"value_function\",\n",
      "            \"target_layer\": ...\n",
      "Processing entity 3/64: value_function\n",
      "Processing entity 4/64: policy\n",
      "Processing entity 5/64: temporal_difference_learning\n",
      "Processing entity 6/64: k_armed_bandit_problem\n",
      "Processing entity 7/64: exploration_exploitation_tradeoff\n",
      "Processing entity 8/64: epsilon_greedy_method\n",
      "Error cleaning JSON: Expecting ',' delimiter: line 23 column 117 (char 912)\n",
      "Problematic text: {\n",
      "    \"relationships\": [\n",
      "        {\n",
      "            \"source\": \"epsilon_greedy_method\",\n",
      "            \"source_layer\": \"algorithm_layer\",\n",
      "            \"target\": \"exploration_exploitation_tradeoff\",\n",
      "            ...\n",
      "Processing entity 9/64: upper_confidence_bound_action_selection\n",
      "Processing entity 10/64: gradient_bandit_algorithm\n",
      "Processing entity 11/64: associative_search\n",
      "Processing entity 12/64: reward_hypothesis\n",
      "Processing entity 13/64: bellman_equation\n",
      "Error cleaning JSON: Expecting ',' delimiter: line 35 column 77 (char 1307)\n",
      "Problematic text: {\n",
      "    \"relationships\": [\n",
      "        {\n",
      "            \"source\": \"bellman_equation\",\n",
      "            \"source_layer\": \"algorithm_layer\",\n",
      "            \"target\": \"markov_decision_process\",\n",
      "            \"target_layer\":...\n",
      "Processing entity 14/64: bellman_optimality_equation\n",
      "Processing entity 15/64: monte_carlo_methods\n",
      "Processing entity 16/64: generalized_policy_iteration\n",
      "Processing entity 17/64: importance_sampling\n",
      "Processing entity 18/64: td_prediction\n",
      "Processing entity 19/64: td_0\n",
      "Processing entity 20/64: sarsa\n",
      "Processing entity 21/64: q_learning\n",
      "Processing entity 22/64: expected_sarsa\n",
      "Processing entity 23/64: double_q_learning\n",
      "Error cleaning JSON: Expecting ',' delimiter: line 47 column 71 (char 1730)\n",
      "Problematic text: {\n",
      "    \"relationships\": [\n",
      "        {\n",
      "            \"source\": \"double_q_learning\",\n",
      "            \"source_layer\": \"algorithm_layer\",\n",
      "            \"target\": \"q_learning\",\n",
      "            \"target_layer\": \"algorithm_...\n",
      "Processing entity 24/64: n_step_td\n",
      "Processing entity 25/64: n_step_sarsa\n",
      "Processing entity 26/64: n_step_expected_sarsa\n",
      "Processing entity 27/64: tree_backup_algorithm\n",
      "Error cleaning JSON: Expecting ',' delimiter: line 25 column 17 (char 963)\n",
      "Problematic text: {\n",
      "    \"relationships\": [\n",
      "        {\n",
      "            \"source\": \"tree_backup_algorithm\",\n",
      "            \"source_layer\": \"algorithm_layer\",\n",
      "            \"target\": \"importance_sampling\",\n",
      "            \"target_layer\"...\n",
      "Processing entity 28/64: n_step_q_lambda\n",
      "Processing entity 29/64: model_of_environment\n",
      "Processing entity 30/64: distribution_model\n",
      "Error cleaning JSON: Expecting ',' delimiter: line 14 column 13 (char 490)\n",
      "Problematic text: {\n",
      "    \"relationships\": [\n",
      "        {\n",
      "            \"source\": \"distribution_model\",\n",
      "            \"source_layer\": \"foundation_layer\",\n",
      "            \"target\": \"model_of_environment\",\n",
      "            \"target_layer\":...\n",
      "Processing entity 31/64: sample_model\n",
      "Processing entity 32/64: planning\n",
      "Error cleaning JSON: Expecting ',' delimiter: line 35 column 63 (char 1300)\n",
      "Problematic text: {\n",
      "    \"relationships\": [\n",
      "        {\n",
      "            \"source\": \"planning\",\n",
      "            \"source_layer\": \"foundation_layer\",\n",
      "            \"target\": \"policy\",\n",
      "            \"target_layer\": \"foundation_layer\",\n",
      "   ...\n",
      "Processing entity 33/64: state_space_planning\n",
      "Error cleaning JSON: Expecting property name enclosed in double quotes: line 47 column 81 (char 1632)\n",
      "Problematic text: {\n",
      "    \"relationships\": [\n",
      "        {\n",
      "            \"source\": \"state_space_planning\",\n",
      "            \"source_layer\": \"foundation_layer\",\n",
      "            \"target\": \"value_function\",\n",
      "            \"target_layer\": \"fo...\n",
      "Processing entity 34/64: dyna_q\n",
      "Processing entity 35/64: on_policy_prediction\n",
      "Processing entity 36/64: approximation_in_rl\n",
      "Processing entity 37/64: episodic_semi_gradient_sarsa\n",
      "Processing entity 38/64: semi_gradient_n_step_sarsa\n",
      "Processing entity 39/64: average_reward_setting\n",
      "Processing entity 40/64: differential_semi_gradient_sarsa\n",
      "Processing entity 41/64: policy_gradient_methods\n",
      "Processing entity 42/64: policy_gradient_theorem\n",
      "Processing entity 43/64: reinforce\n",
      "Processing entity 44/64: actor_critic_methods\n",
      "Processing entity 45/64: policy_parameterization\n",
      "Processing entity 46/64: soft_max_policy_parameterization\n",
      "Processing entity 47/64: gaussian_policy_parameterization\n",
      "Processing entity 48/64: psychology_domain\n",
      "Processing entity 49/64: classical_conditioning\n",
      "Processing entity 50/64: instrumental_conditioning\n",
      "Processing entity 51/64: td_model\n",
      "Processing entity 52/64: rescorla_wagner_model\n",
      "Processing entity 53/64: dopamine\n",
      "Processing entity 54/64: td_error\n",
      "Processing entity 55/64: actor_critic\n",
      "Error cleaning JSON: Expecting ',' delimiter: line 48 column 42 (char 1782)\n",
      "Problematic text: {\n",
      "    \"relationships\": [\n",
      "        {\n",
      "            \"source\": \"actor_critic\",\n",
      "            \"source_layer\": \"algorithm_layer\",\n",
      "            \"target\": \"policy\",\n",
      "            \"target_layer\": \"Foundation Layer\",\n",
      "...\n",
      "Processing entity 56/64: td_gammon\n",
      "Processing entity 57/64: alpha_go\n",
      "Processing entity 58/64: deep_q_network\n",
      "Processing entity 59/64: monte_carlo_tree_search\n",
      "Error cleaning JSON: Expecting ',' delimiter: line 47 column 52 (char 1601)\n",
      "Problematic text: {\n",
      "    \"relationships\": [\n",
      "        {\n",
      "            \"source\": \"monte_carlo_tree_search\",\n",
      "            \"source_layer\": \"algorithm_layer\",\n",
      "            \"target\": \"monte_carlo_methods\",\n",
      "            \"target_laye...\n",
      "Processing entity 60/64: general_value_functions\n",
      "Error cleaning JSON: Expecting ',' delimiter: line 35 column 54 (char 1379)\n",
      "Problematic text: {\n",
      "    \"relationships\": [\n",
      "        {\n",
      "            \"source\": \"general_value_functions\",\n",
      "            \"source_layer\": \"foundation_layer\",\n",
      "            \"target\": \"value_function\",\n",
      "            \"target_layer\": ...\n",
      "Processing entity 61/64: auxiliary_tasks\n",
      "Processing entity 62/64: options\n",
      "Error cleaning JSON: Expecting ',' delimiter: line 23 column 87 (char 841)\n",
      "Problematic text: {\n",
      "    \"relationships\": [\n",
      "        {\n",
      "            \"source\": \"options\",\n",
      "            \"source_layer\": \"foundation_layer\",\n",
      "            \"target\": \"markov_decision_process\",\n",
      "            \"target_layer\": \"founda...\n",
      "Processing entity 63/64: partial_observability\n",
      "Processing entity 64/64: reward_signals\n",
      "\n",
      "Relationship extraction complete!\n",
      "Total relationships found: 252\n",
      "\n",
      "Layer Statistics:\n",
      "- foundation_layer: 25/31 entities connected\n",
      "- method_layer: 3/3 entities connected\n",
      "- algorithm_layer: 22/28 entities connected\n",
      "- application_layer: 2/2 entities connected\n",
      "\n",
      "Layer Connections:\n",
      "- up: 125 relationships\n",
      "- down: 27 relationships\n",
      "- same: 97 relationships\n",
      "- across: 3 relationships\n",
      "\n",
      "Results saved to relationships.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import Dict, List\n",
    "from openai import OpenAI\n",
    "\n",
    "class LayeredRelationshipExtractor:\n",
    "    def __init__(self, api_key: str = None):\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "            api_key=api_key or \"paste your api key here\"\n",
    "        )\n",
    "\n",
    "    def determine_layer(self, entity_data: Dict) -> str:\n",
    "        \"\"\"Determine which layer an entity belongs to based on its properties.\"\"\"\n",
    "        if 'type' in entity_data:\n",
    "            entity_type = entity_data['type'].lower()\n",
    "            \n",
    "            # Mathematical and theoretical concepts\n",
    "            if entity_type in ['theorem', 'equation', 'principle', 'proof', 'definition', \n",
    "                             'framework', 'concept', 'property', 'space', 'function']:\n",
    "                return 'foundation_layer'\n",
    "            \n",
    "            # Methods and approaches\n",
    "            elif entity_type in ['value_based', 'policy_based', 'model_based', 'hybrid', 'method']:\n",
    "                return 'method_layer'\n",
    "            \n",
    "            # Algorithms and implementations\n",
    "            elif entity_type in ['algorithm', 'base_algorithm', 'variant', 'improvement', 'extension']:\n",
    "                return 'algorithm_layer'\n",
    "            \n",
    "            # Applications and domains\n",
    "            elif entity_type in ['field', 'benchmark', 'use_case', 'environment', 'task', 'domain']:\n",
    "                return 'application_layer'\n",
    "        \n",
    "        # Default to foundation layer if unclear\n",
    "        return 'foundation_layer'\n",
    "\n",
    "    def create_relationship_prompt(self, entity_id: str, entity: Dict, all_entities: Dict) -> str:\n",
    "        # Determine the layer of the source entity\n",
    "        source_layer = self.determine_layer(entity)\n",
    "        \n",
    "        # Create layer-specific entity groupings\n",
    "        entities_by_layer = {\n",
    "            'foundation_layer': [],\n",
    "            'method_layer': [],\n",
    "            'algorithm_layer': [],\n",
    "            'application_layer': []\n",
    "        }\n",
    "        \n",
    "        for eid, e in all_entities.items():\n",
    "            if eid != entity_id:\n",
    "                layer = self.determine_layer(e)\n",
    "                entities_by_layer[layer].append({\n",
    "                    'id': eid,\n",
    "                    'name': e['name'],\n",
    "                    'type': e.get('type', '')\n",
    "                })\n",
    "\n",
    "        return f\"\"\"Analyze this entity and identify its relationships with other entities, considering their respective layers in the RL knowledge hierarchy.\n",
    "\n",
    "SOURCE ENTITY (from {source_layer}):\n",
    "ID: {entity_id}\n",
    "Name: {entity['name']}\n",
    "Type: {entity.get('type', '')}\n",
    "Definition: {entity.get('definition', '')}\n",
    "Properties: {json.dumps(entity.get('properties', []), indent=2)}\n",
    "Source: {json.dumps(entity.get('source', {}), indent=2)}\n",
    "\n",
    "POTENTIAL TARGET ENTITIES BY LAYER:\n",
    "\n",
    "Foundation Layer (Mathematical & Theoretical Concepts):\n",
    "{json.dumps(entities_by_layer['foundation_layer'], indent=2)}\n",
    "\n",
    "Method Layer (Approaches):\n",
    "{json.dumps(entities_by_layer['method_layer'], indent=2)}\n",
    "\n",
    "Algorithm Layer (Implementations):\n",
    "{json.dumps(entities_by_layer['algorithm_layer'], indent=2)}\n",
    "\n",
    "Application Layer (Domains & Use Cases):\n",
    "{json.dumps(entities_by_layer['application_layer'], indent=2)}\n",
    "\n",
    "Return ONLY valid JSON that follows this format exactly:\n",
    "{{\n",
    "    \"relationships\": [\n",
    "        {{\n",
    "            \"source\": \"{entity_id}\",\n",
    "            \"source_layer\": \"{source_layer}\",\n",
    "            \"target\": \"target_entity_id\",\n",
    "            \"target_layer\": \"layer_name\",\n",
    "            \"type\": \"descriptive_relationship_type\",\n",
    "            \"direction\": \"up|down|same|across\",\n",
    "            \"evidence\": {{\n",
    "                \"text\": \"exact text snippet that shows this relationship\",\n",
    "                \"location\": \"definition|property|source\"\n",
    "            }}\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "IMPORTANT:\n",
    "1. Only extract relationships that are explicitly stated in the text\n",
    "2. Use specific, descriptive relationship types based on the actual content\n",
    "3. Always include the supporting evidence\n",
    "4. Note the direction:\n",
    "   - \"up\" for relationships to higher layers\n",
    "   - \"down\" for relationships to lower layers\n",
    "   - \"same\" for relationships within the same layer\n",
    "   - \"across\" for relationships that cross layers non-hierarchically\n",
    "5. Consider layer-appropriate relationships:\n",
    "   - Foundation → Method: \"enables\", \"provides basis for\"\n",
    "   - Method → Algorithm: \"is implemented by\", \"guides\"\n",
    "   - Algorithm → Application: \"is applied to\", \"solves\"\n",
    "   - Same layer: \"relates to\", \"extends\", \"similar to\"\n",
    "   - Cross-layer: \"inspired by\", \"analogous to\"\n",
    "\"\"\"\n",
    "\n",
    "    def clean_json_response(self, response_text: str) -> dict:\n",
    "        \"\"\"Clean and parse JSON from API response.\"\"\"\n",
    "        try:\n",
    "            # Try to find JSON between ```json and ``` markers\n",
    "            import re\n",
    "            json_block = re.search(r'```json\\s*(.*?)\\s*```', response_text, re.DOTALL)\n",
    "            if json_block:\n",
    "                response_text = json_block.group(1)\n",
    "            else:\n",
    "                # If no code blocks, try to find content between { and }\n",
    "                json_match = re.search(r'(\\{.*\\})', response_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    response_text = json_match.group(1)\n",
    "\n",
    "            return json.loads(response_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error cleaning JSON: {e}\")\n",
    "            print(f\"Problematic text: {response_text[:200]}...\")\n",
    "            return {\"relationships\": []}\n",
    "\n",
    "    def extract_relationships(self, input_file: str):\n",
    "        \"\"\"Extract relationships for all entities while preserving layer information.\"\"\"\n",
    "        # Read input file\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            entities = data['entities']\n",
    "\n",
    "        all_relationships = []\n",
    "        layer_statistics = {\n",
    "            'foundation_layer': {'total': 0, 'connected': 0},\n",
    "            'method_layer': {'total': 0, 'connected': 0},\n",
    "            'algorithm_layer': {'total': 0, 'connected': 0},\n",
    "            'application_layer': {'total': 0, 'connected': 0}\n",
    "        }\n",
    "\n",
    "        # Process each entity\n",
    "        total_entities = len(entities)\n",
    "        for i, (entity_id, entity) in enumerate(entities.items(), 1):\n",
    "            print(f\"Processing entity {i}/{total_entities}: {entity_id}\")\n",
    "            \n",
    "            # Track layer statistics\n",
    "            entity_layer = self.determine_layer(entity)\n",
    "            layer_statistics[entity_layer]['total'] += 1\n",
    "            \n",
    "            try:\n",
    "                # Generate prompt for this entity\n",
    "                prompt = self.create_relationship_prompt(entity_id, entity, entities)\n",
    "                \n",
    "                # Get relationships from LLM\n",
    "                completion = self.client.chat.completions.create(\n",
    "                    model=\"nvidia/llama-3.1-nemotron-70b-instruct\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0.3,\n",
    "                    max_tokens=2048\n",
    "                )\n",
    "                \n",
    "                if completion.choices:\n",
    "                    response_text = completion.choices[0].message.content\n",
    "                    extracted = self.clean_json_response(response_text)\n",
    "                    \n",
    "                    if 'relationships' in extracted and extracted['relationships']:\n",
    "                        layer_statistics[entity_layer]['connected'] += 1\n",
    "                        all_relationships.extend(extracted['relationships'])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing entity {entity_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        unique_relationships = []\n",
    "        for rel in all_relationships:\n",
    "            rel_key = (rel['source'], rel['target'], rel['type'])\n",
    "            if rel_key not in seen:\n",
    "                seen.add(rel_key)\n",
    "                unique_relationships.append(rel)\n",
    "\n",
    "        # Analyze layer connections\n",
    "        layer_connections = {\n",
    "            'up': 0,\n",
    "            'down': 0,\n",
    "            'same': 0,\n",
    "            'across': 0\n",
    "        }\n",
    "        for rel in unique_relationships:\n",
    "            if 'direction' in rel:\n",
    "                layer_connections[rel['direction']] += 1\n",
    "\n",
    "        # Save to file\n",
    "        output = {\n",
    "            \"relationships\": unique_relationships,\n",
    "            \"metadata\": {\n",
    "                \"total_relationships\": len(unique_relationships),\n",
    "                \"relationship_types\": sorted(list(set(rel['type'] for rel in unique_relationships))),\n",
    "                \"total_entities_involved\": len(set(\n",
    "                    entity_id \n",
    "                    for rel in unique_relationships \n",
    "                    for entity_id in [rel['source'], rel['target']]\n",
    "                )),\n",
    "                \"layer_statistics\": layer_statistics,\n",
    "                \"layer_connections\": layer_connections\n",
    "            }\n",
    "        }\n",
    "\n",
    "        with open('relationships.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(output, f, indent=2)\n",
    "\n",
    "        return output\n",
    "\n",
    "def main():\n",
    "    extractor = LayeredRelationshipExtractor()\n",
    "    input_file = r\"rl_kg\\output\\rl_knowledge_graph.json\"\n",
    "    \n",
    "    try:\n",
    "        result = extractor.extract_relationships(input_file)\n",
    "        print(f\"\\nRelationship extraction complete!\")\n",
    "        print(f\"Total relationships found: {result['metadata']['total_relationships']}\")\n",
    "        print(\"\\nLayer Statistics:\")\n",
    "        for layer, stats in result['metadata']['layer_statistics'].items():\n",
    "            print(f\"- {layer}: {stats['connected']}/{stats['total']} entities connected\")\n",
    "        print(\"\\nLayer Connections:\")\n",
    "        for direction, count in result['metadata']['layer_connections'].items():\n",
    "            print(f\"- {direction}: {count} relationships\")\n",
    "        print(\"\\nResults saved to relationships.json\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during relationship extraction: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
