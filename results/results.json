{
  "pdf_name": "tmpt0g55nxe",
  "pages": [
    {
      "page_num": 1,
      "figures": [],
      "tables": [],
      "text": "arXiv:1802.00332v1 [cs.AI] 1 Feb 2018\n\nElements of Effective Deep Reinforcement Learning\ntowards Tactical Driving Decision Making\n\nJingchu Liu, Pengfei Hou, Lisen Mu, Yinan Yu, and Chang Huang\nHorizon Robotics, inc.\n{jingchu.liu, pengfei01.hou, lisen.mu, yinan.yu, chang.huang} @hobot.cc\n\nAbstract\n\nTactical driving decision making is crucial for au-\ntonomous driving systems and has attracted con-\nsiderable interest in recent years. In this paper,\nwe propose several practical components that can\nspeed up deep reinforcement learning algorithms\ntowards tactical decision making tasks: 1) non-\nuniform action skipping as a more stable alternative\nto action-repetition frame skipping, 2) a counter-\nbased penalty for lanes on which ego vehicle has\nless right-of-road, and 3) heuristic inference-time\naction masking for apparently undesirable actions.\nWe evaluate the proposed components in a realistic\ndriving simulator and compare them with several\nbaselines. Results show that the proposed scheme\nprovides superior performance in terms of safety,\nefficiency, and comfort.\n\n1 Introduction\n\nAutonomous driving has attracted considerable interest in the\npast two decades and significant progress has been achieved.\nAccording to Douges (Donges, 1999}, autonomous driving\ntasks can be roughly classified into three categories: naviga-\ntion, guidance, and stabilization. Strategic navigation tasks\nare responsible for generating road-level routes. Tactical-\nlevel guidance tasks are responsible for guiding ego vehi-\ncle along these routes in complex environments by generat-\ning tactical maneuver decisions. And operational-level stabi-\nlization tasks are responsible for translating tactical decisions\ninto reference trajectories and then low-level control signals.\nAmong these three classes of tasks, tactical-level decision\nmaking is especially important due to its central role and has\nbeen an active field of research.\n\nEarly successes of decision making systems typically rely\non human-designed rules to control the decision process, us-\ning methods such as heuristic rules, decision trees, finite\nstate machines, or\nUrmson et al., 2008} Miller et al., 8]. These methods are\noften tailored for specific environments and do not generalize\nrobustly.\n\nMore recently, the problem of tactical decision making has\nbeen cast into the Partially Observable Markov Decision Pro-\ncess (POMDP) framework and various approximate methods\n\nhave been proposed to solve the theoretically intractable mod-\n\nels for tactical decision making [Ulbrich and Maurer, 2013\nBrechtel et al., 2014} |Galceran et al., 2015]. One common\n\nproblem faced with POMDP-based work is the strong depen-\ndency to a relatively simple environment model, usually with\ndelicately hand-crafted (discrete) observation spaces, transi-\ntion dynamics, and observation mechanisms. These strong\nassumptions limit the generality of these methods to more\ncomplex scenarios.\n\nIn recent years, the success of deep learning has revived the\ninterest in end-to-end driving agent which decides low-level\n\ncontrol directly from image inputs, using supervised learni\nBojarski et al., 2017] or reinforcement learning (RL) [Sallab\nlet al., 2017} |Plessen, 2017]. But the black-box driving poli-\n\ncies learned by these methods are susceptible to influence un-\nder drifted inputs. Although efforts have been made to iden-\ntify a more robust and compact subset of prediction targets\nthan control outputs (e.g. in (Chen er al., 2015), most prac-\ntical autonomous driving systems to date still only use deep\nlearning as a restricted part of the whole system.\n\nDeep RL is a natural way to incorporate deep learning\ninto traditional POMDP or RL-based decision making meth-\nods. The use of function approximators makes it possible\nto directly use high-dimensional raw observations. This al-\n\nleviates the strong dependency to hand-crafted simple mod-\nels in traditional POMDP and RL-based work. Along this\n\nline of research, [Isele et al., 2017) and\njal., 2017] apply the deep Q-network (DQN) [Mnih ez al.,|\n\n[2013] to learn tactical decision policies for intersection cross-\ning and lane changing on freeway, respectively. Hierarchi-\ncal RL is combined with Monte-Carlo tree search (MCTS)\n\nin to simultaneously learn a high-\nlevel option policy for decision making and a low-level pol-\nicy for option execution. Shalev-Shwartz ef al., 2016}\ncombine a high-level RL pol-\nicy with a non-learnable low-level policy to balance between\nefficiency and safety.\n\nHowever, many commonly-used techniques for deep RL\nare originally proposed for low-level control tasks and can\nbe less efficient and/or robust for high-level tactical decision\nmaking from our observation. Firstly for temporal abstrac-\ntion, frame skipping with action repetition will cause unstable\nlow-level behavior due to the discontinuity in high-level ac-\ntion semantics. Secondly for the multi-dimensional reward-\n\n"
    },
    {
      "page_num": 2,
      "figures": [
        {
          "block": {
            "type": "figure",
            "path": "results\\figures\\figure_2.png",
            "y_position": 155,
            "confidence": 0.995757520198822
          },
          "coords": [
            877,
            155,
            1548,
            611
          ],
          "file_path": "results\\figures\\figure_2.png"
        }
      ],
      "tables": [],
      "text": "ing systems, we find that the commonly used sparse global\ngoal indicators and dense local goal metrics are in general\nredundant and harmful, and a constant value lane penalty is\nhard to induce favorable lane switching behavior in multi-lane\nscenarios. Thirdly, decision agents relying mere on a learned\nRL policy may accidentally issue disastrous action under the\ninfluence of observation noise.\n\nIn this paper, we aim to tackle the above difficulties for\ndeep RL. Our main contributions are a set of practical yet\neffective elements for deep RL agents towards tactical driv-\ning decision making tasks. We propose non-uniform action\nskipping as a more stable alternative to action repetition. A\ncounter-based lane penalty is also proposed to encourage de-\nsired behavior for multi-lane scenarios. During inference, the\nlearned RL agent can be further enhanced with heuristic rules\nthat filter out obviously undesirable actions. These elements\nare meant to make as less modification to existing methods as\npossible for the sake of simplicity, and target the peculiarity\nof high-level tactical decision making for effectiveness. The\nproposed elements are equipped in a hierarchical autonomous\ndriving system and the effectiveness is demonstrated in realis-\ntic driving scenarios presenting two-way traffic and signaled\nintersections.\n\n2 Method\n\nWe consider a hierarchical autonomous driving system that\norchestrates learning-based and non-learning modules: the\ntactical decision making module is implemented as a deep RL\nagent for efficiency while the routing, planning, and control\nmodules are realized with non-learning methods for safety\nand comfort. The routing module calculates local lane sug-\ngestions towards the global goal according to a road map.\nThe decision module takes into consideration both the rout-\ning suggestions and other information such as the status of\nego vehicle and road structure to make high-level maneuver\ndirections. The planning module then translates those direc-\ntions into the trajectories of vehicle poses. The control mod-\nule finally implements the planning trajectories into low-level\ncontrol signals. Note the planning module has certain built-in\nsafety functions to avoid hazards such as collisions.\n\nFrom a agent-centric perspective, the tactical decision\nagent makes sequential decisions in the dynamic environ-\nment composed of all non-learning modules and the rest of\nthe world. In time step t, information about ego vehicle and\nthe surrounding environment is compiled into an observation\no, and presented to the agent. The agent then selects a tacti-\ncal decision action a; = 7(0;) according to the decision pol-\nicy 7(-). Downstream modules will receive this decision and\ncontrol the movement of ego vehicle accordingly. A reward-\ning module will then assess the movement in current time step\nto provide a scalar reward r,. And the system will evolve for-\nward in time into the next time step t+ 1. The goal is to learn\na policy that maximizes the expected total discounted reward\n\nT\nE{Gi} -2{Soomtad. (1)\nT=t\n\nNote the world state is only partially observable to the de-\ncision agent due to imperfect sensing and unpredictable be-\n\nFigure 1: Illustration for the discontinuous action semantics of lane\nswitching tasks: the meaning of “switching to right lane” changes\nafter crossing the lane marking.\n\nhavior of other agents. Therefore, we extend the observa-\ntion vector 0; into history by means of frame stacking. Other\nmethods, e.g. recursive neural networks, can also be used to\nfill more information into the observation vector.\n\n2.1 Action Skipping\nAction-repeated frame skipping |Mnih er al., 2013] is a\n\ncommonly-used technique to speed up reinforcement learn-\ning algorithms. The benefits are multi-fold. For explo-\nration, non-trivial high-level maneuvers can be more easily\nexplored with random perturbation. Otherwise the difficulty\nof forming a series of low-level movements that correspond\nto a high-level maneuver can be exponential in the length of\nthat series. Also, the effective horizon of the semi-MDP re-\nsulting from action repetition is proportionally shorter than\nthe original MDP. And bootstrapped value estimation meth-\nods, such as temporal difference learning, will receive pro-\nportional speedup. Moreover, the reward signal can become\nmore resilient to noises and delays thanks to the extended ef-\nfective period of each action.\n\nHowever, action repetition can be less stable for high-level\ndecision making tasks due to the discontinuity in action se-\nmantics. Consider driving on a multi-lane road shown in Fig-\nure [I] when ego vehicle is just about to cross the marking\nbetween the current lane (Lo) and the lane immediately to\nthe right (Lj), the action switching to the right lane means\nswitching from Lo to L;. But after ego car has crossed the\nlane marking, the semantics of that same action is radically\nchanged to switching from L, to L2, in which Lz is the lane\nfurther to the right of L,. If the frame skipping period con-\ntains such moments of non-continuous action semantics, the\nresulting high-level direction is doomed to result in unfavor-\nable lower-level behaviors, e.g. frequent overshot and correc-\ntion for the above scenario.\n\nWe propose to skip instead of repeat actions. Concretely,\neach agent action is now a meta action consisting of an actual\ndecision followed by several null actions (No-Op). We de-\nnote this operation as action skipping. During skipping, the\n\n"
    },
    {
      "page_num": 3,
      "figures": [],
      "tables": [],
      "text": "agent can continue to collect observations and rewards. The\noverall reward for the meta action is then calculated as the\naverage of all rewards collected during the skipping period.\nNote to implement action skipping in the decision module,\nthe lower-level modules need to continue operation under the\nnull action. This is not a problem as long as the trajectory\nplanning module plans over a horizon longer than the skip-\nping period, which is relatively easy to implement.\n\nOne drawback of action skipping is the decrease in deci-\nsion frequency which will delay or prevent the agent’s reac-\ntion to critical events. To improve the situation, the actions\ncan take on different skipping factors during inference. For\ninstance in lane changing tasks, the skipping factor for lane\nkeeping can be kept short to allow for swift maneuvers while\nthe skipping factor for lane switching can be larger so that the\nagent can complete lane changing actions. When perform-\ning non-uniform action skipping, the agent may observe time\nphases that are skipped during training and cause domain drift\nin observations. As a solution, we uniformly randomly extend\nthe skipping factor of the first agent step by a factor between\nzero and one so that the agent can observe all possible time\nphases during training.\n\n2.2 Reward Function\n\nTactical decision making needs to balance among efficiency,\nsafety, and comfort. Therefore the scalar reward used for tac-\ntical decision making is usually composed of multiple com-\nponents, most often through linear combination. Roughly\nspeaking, these reward signals can be classified into sparse\nglobal goal indicators, sparse constraint violation alerts, and\ndense heuristic components. The reward components we use\nis shown in Table[I]and our choices are explained below.\n\nGlobal Goal Indicators\nGlobal goal indicators are very sparse signals that only take\non non-zero values when a long-term goal if achieved or\nmissed. For tactical decision making, the true long-term goal\nis reaching the destination given by the navigation module as\nfast as possible. Therefore the most common form of global\ngoal indicators is a signal given at the end of each episode,\npositive if ego car reaches the destination and negative other-\nwise. In this way, the discounted total reward will be larger\nfor episodes in which ego car reach the destination earlier.\nWe argue that global goal indicators are not only unnec-\nessary but also burdens to the tactical decision making agent.\nThe preferences described by global goal indicators (i.e. what\nis wanted) can be implicitly expressed with some denser re-\nciprocal constraint violation alerts (i.e. what is unwanted).\nThis is possible because all behaviors that will stop ego ve-\nhicle from reaching the destination can be defined as vio-\nlating constraints and harshly penalized. The use of dense\nper-step cost can also further devalue behaviors that will mis-\nlead ego vehicle into dead-end situations. As a result, the\nbehavior that will help achieve a global goal will naturally\nresult in low penalty. In comparison with sparse indicators,\ntheir denser counterparts will usually result in faster credit\nassignment during value estimation and is therefore more\ndesirable. Moreover, global goal indicators emitted at the\nend of episodes will generally increase the absolute value\n\nof expected accumulated rewards in comparison with the\nindicator-free counterpart. In turn, the approximators used\nfor value functions needs to have larger dynamic range, which\nmean more learning steps during training and larger variance\nduring inference. For these reasons we do not use any global\ngoal indicator components in our experiments.\n\nConstraint Violation Alerts\n\nConstraint violation alerts are sparse signals that penalize the\nagent for being in a risky situation or performing a risky ac-\ntion. The most common situation considered is collision with\nother road objects, e.g. other vehicles or pedestrian. Note re-\nward signals that fall into this category need not to be sparse\nduring constraint violations events. They are sparse in the\nsense that risky situation should be very rare under a properly\ncautious driving policy.\n\nWe consider three types of risky situations in our experi-\nments: 1) entering intersection during red light, 2) collision\nwith other vehicles, and 3) treading onto biking and opposite\nlanes on which ego vehicle has less priority. The former two\ncomponents will also termination the episode.\n\nTraffic light: The traffic light alert is triggered when ego\nvehicle enters an intersection when the corresponding con-\nnection road is covered by a red light. Note although we ren-\nder longitudinal speed control to a rule-based planning mod-\nule and it will automatically stop ego vehicle in most situa-\ntions, there are still corner cases that may accidentally grant\nego vehicle’s access into intersection during red lights. There-\nfore a traffic light alert should be in place to penalize these\ncorner-case behaviors.\n\nCollision risk: The collision risk component is active when\nego vehicle is about to crash into other vehicles. It is the sum\nof risk components contributed by each of the other vehicles\nin the region of interest. Each component is further defined\nas the product of an isotropic distance-based factor and a di-\nrectional factor related to the heading direction of cars:\n\nre= Dor ry 2)\n\nwhere ri is the isotropic factor by target 7 and ri is the direc-\ntional factor. The distance-based risk factor is Laplacian in\nthe distance between the target car and ego car:\n\nn(d) =e, (3)\n\nwhere ri(d) is the distance-based risk for target car i at dis-\ntance d and dp is a normalizing distance. The directional risk\nfactor is the product of two narrow-band raised-cosine pat-\nterns, the center of which are aligned with the heading direc-\ntion of ego car and the target car, respectively:\n\n7 (Peg0, Frarger) = TCOS(A — Fogo)  TCOS(A — Prarger), (4)\n\nwhere rcos(-) is the narrow-band raised cosine function, Beso\nand Otarget are the heading angle of ego and target vehicle, and\n@ is the direction of the vector connecting ego car and target\ncar. The overall effect of these two factors is high risk only\nwhen ego car and a target car is about to drive head-to-head\n"
    },
    {
      "page_num": 4,
      "figures": [
        {
          "block": {
            "type": "figure",
            "path": "results\\figures\\figure_4.png",
            "y_position": 147,
            "confidence": 0.9852129220962524
          },
          "coords": [
            187,
            147,
            1498,
            461
          ],
          "file_path": "results\\figures\\figure_4.png"
        }
      ],
      "tables": [],
      "text": "lable 1: Description tor reward components and their weights.\n\ninto each other. Otherwise the risk is relatively low, e.g. when\nego car and the target car is driving closely side-by-side.\n\nAnother commonly-used measure for crashing risk is\nTime-to-Collision (TTC). TTC is roughly inversely propor-\ntional to the distance between ego and the target car. There-\nfore it is relatively aggressive in risk prediction and tend to\nexaggerate crashing risk. In contrast, the proposed risk for-\nmulation indicates risk only when it is about to happen and\nis thus more conservative. To foresee upcoming hazards, the\nagent can use common value estimation methods to predict\nfuture risk values.\n\nDangerous lane: We use counter-based risk signals to re-\nflect the ever-increasing empirical risk of staying on biking\nand opposite lanes. Specifically, the indicator for each lane\nmaintains a counter (with a maximum value cap) that keeps\ntrack of the time steps that ego vehicle has spent on that lane.\nThe risk value is then computed as a linear function of the\ncorresponding counter value:\n\nR= (01a +0.9) x (a > 1.0), (5)\n\nwhere R is the risk value and x is the counter value. The lane\nrisk defined as such will be relatively small when ego vehi-\ncle just arrived on a dangerous lane and will gradually be-\ncome intolerably large if ego vehicle remains there for a long\ntime. In this way, temporally switching onto dangerous lanes,\nwhich is required for overtaking slower traffic on single lane\nroads, can be enabled. And staying on dangerous lane will be\nprohibited in the long run. Note since the reward component\ndefined in this way becomes stateful, it is important to aug-\nment agent observations with history information so that it\ncan roughly infer how long it has stayed on a particular lane.\n\nIn contrast, it is much more difficult to design constant-\nvalue risk signals that has the same enabling effect on over-\ntake maneuvers: a small risk value may encourage ego vehi-\ncle to switch onto dangerous lanes when necessary, but it may\neasily fail on encouraging backward lane switching because\nthe small difference in risk value can be easily overwhelmed\nby the variance in approximated value functions. The agent\ncan only slowly learn the risk of dangerous lanes from sparse\ncollision events. Meanwhile, a large risk value will in effect\nprohibit switching onto dangerous lanes, even when doing so\nis beneficial.\n\nDense heuristic metrics\nReward signals belonging to this category are usually used\nto hard-code designer’s heuristic preference for some states\n\nand actions. Unlike the former two reward categories, which\nare easier to design as they can reflect orthogonal aspects of\ndesired and unwanted outcomes, dense heuristic metrics are\nharder to design because heuristic rules can easily conflict\nwith each other or, even worse, fail to align with the global\ngoals. For this reason, we aim to employ only a minimal set\nof dense heuristic components.\n\nWe consider a component proportional to the speed of ego\nvehicle along the navigation route to encourage driving to-\nwards the goal as fast as possible. The speed limits are mon-\nitored by the planning module to avoid over-speeding. The\nsecond dense component we consider is a per-step penalty\nfor lane changing actions to discourage unnecessary lateral\nmaneuvers and improve passenger comfort. The final dense\ncomponent applied is a trivial per-step cost to prefer short\nepisodes. We do not employ any dense penalties related to\nlocal goals (e.g. local target lane or headway to other vehi-\ncles) as they can easily conflict with other heuristic metrics\nand the global goal.\n\n2.3. Rule-based Action Masking\n\nIn some scenarios, undesirable tactical actions can be\nstraightforwardly identified. In such cases, we proposed to\napply simple rules to filter out those actions during inference\ninstead of only hoping the agent to learn to avoid those ac-\ntions. The reason is that, on one hand, even if the agent can\nlearn to avoid inferior actions, they can still be triggered due\nto the variance in observation and the learned model. On the\nother hand, those simple rules designed for straightforward\nsituations are less prone to unexpected false positives and eas-\nier to debug if any happens. This is in contrast to conventional\nrule-based decision policies which is comprised of a complex\nset of rules intended to work under complex scenarios.\n\n3 Implementation\n\nAs shown in Figure the decision agent observes tilted\nRGB top-down views of ego vehicle’s immediate surround-\ning. In each time step, the latest two frames from the ren-\ndered 10Hz image stream are max-pooled pixel-wise into a\nsingle image frame to combat flickering. The frames from\nthe latest three time steps are further stacked channel-wise to\nform a single observation. The reward components and lin-\near combination weights defined in Section |2.2] are used to\nderive a scalar reward function. For comparison, some of the\ncomponents may be removed or replaced for comparison.\n\n"
    },
    {
      "page_num": 5,
      "figures": [
        {
          "block": {
            "type": "figure",
            "path": "results\\figures\\figure_5.png",
            "y_position": 146,
            "confidence": 0.988714873790741
          },
          "coords": [
            152,
            146,
            828,
            518
          ],
          "file_path": "results\\figures\\figure_5.png"
        }
      ],
      "tables": [],
      "text": "Figure 2: (a) Illustration of the tilted top-down view used for agent\nobservation. (b) Simulation scenario with signaled intersection and\ntwo-way traffic. Red line indicates the navigation route.\n\nThe agent is composed of a dueling deep Q-network\n(DQN) {Wang eral. 2015], with 3 convolutional layers fol-\nlowed by 2 dense layers and one linear output layer. The last\ndense layer is divided into an action-wise advantage channel\nand a baseline channel. The Q value for each action is the\nsum of its corresponding advantage plus the shared baseline.\nThe three convolutional layers has 16, 16, and 64 kernels with\nsize 8, 5, 3 and stride 4, 3, and 2, respectively. The first dense\nlayer has 256 hidden units while the latter dense layer has 256\nhidden units for each of the two channels. All convolutional\nlayers apply 3 x 3 max pooling and all hidden layers apply\nReLU activation.\n\nThe discount factor used is 0.9 and double Q learning\n\nHasselt et al., 2015] is applied to build up the temporal differ-\n\nence loss. We use a mini-batch of 8 samples and an ADAM\noptimizer with learning rate le~*, 6; = 0.9, and B2 = 0.999\nto train the learning network. The target network uses a syn-\nchronization rate of 1le~* to track the learning network. The\nexploration factor is annealed linearly from 0.2 to 0.05 in\n30K steps. The training process is handled asynchronously\nby a rate-controlled training thread: for each data point col-\nlected from the environment, this thread can perform 8 up-\ndates for the learning network. The training thread samples\nfrom a memory-mapped on-disk replay buffer of maximum\nsize 300K. The data is divided into consecutive fragments of\nsize 100 and about 30 fragments are cached in memory for\nrandom sampling at any time. Each fragment will be sampled\nfor at most 200 times and a new fragment will be swapped in\nfor replacement.\n\n4 Experimental Results\n\nAs shown in Fig 2b] we experiment with different agent con-\nfigurations in simulated driving scenarios with two-way traf-\nfic and signaled intersections. The simulator is wrapped as an\n\nRL environment complying to the OpenAI Gym API\n\nman et al., 2016] and the frequency of agent-environment in-\n\nteraction is regularized to 2Hz in terms of simulation clock.\nAgents are trained and tested with episodic simulation. In\neach episode, a route is sampled randomly from the road map\nof 20 routes. Each route consists of two road segments con-\nnected by an intersection. A number of vehicles are scat-\n\ntered along this route with random starting points and cruising\nspeed. Ego vehicle will then start from the beginning of the\nselected route. A global navigation module will constantly\nprovide a reference lane (rendered with a red line in observed\nimage) that leads to the route destination. An episode is ter-\nminated if ego vehicle encounters any of the following con-\nditions: 1) reaching destination; 2) receiving a crashing risk\nvalue > 1.0; 3) stop moving for 40 secondg\"| 4) entering in-\ntersection on unpermitted lanes. Each agent configuration is\ntrained from scratch for 10 simulation runs. Each run ter-\nminates once the accumulated number of environment steps\nexceeds 250K. The trained agents are then evaluated with-\nout exploration against 100 pre-generated test episodes. The\nnumber of other vehicles present in each episode is 32 for\nboth training and test.\n\nTest results are shown in Table[2|and organized into 4 sec-\ntions. We select success rate, longitudinal speed, and lateral\nspeed as the performance metrics to reflect safety, efficiency,\nand comfort. The agent needs to switch to the correct lane and\navoid collisions to successfully finish each episode. Overtak-\ning slower traffic is also required to achieve a high longitu-\ndinal speed. Moreover, the agent needs to avoid unnecessary\nlateral maneuvers in order to reduce lateral speed. The over-\nall metric for each configuration is calculated as the median\nper-step metric of all 10 simulation runs.\n\nThe first section presents the random and rule-based base-\nlines. The simulation scenario is complex enough such that\na random policy can only achieve a success rate of 8%. And\nthe rule-based decision agent can improve the success rate\nmetric to a more reasonable number of 79.0%. The longitu-\ndinal speed of rule-based of agent is higher than the random\nagent thanks to overtake maneuvers. And the lateral speed is\nreduced because a reasonable decision policy will avoid lane\nchanging and stay on the current lane for most of the time.\n\nThe second section compares different skipping configura-\ntions. The RL agent without any skipping operation (ID03)\nperforms rather poor, achieving a success rate of only 29.5%.\nAction repetition operations (ID04) can help significantly im-\nprove the success rate to 83%. This demonstrates the impor-\ntance of temporal abstraction to high-level tactical decision\nmaking tasks. The success rate of dynamic action skipping\nscheme (ID05) is surprisingly unsatisfactory even though its\naction space is a super-set to the action repetition scheme.\nOne possible explanation may be that the gains from broad-\nening the action space is over-weighed by the disadvantage of\nextending the effective horizon . As expected, action skipping\n(ID10) will result in smaller lateral speed than action repeti-\ntion (ID04) thanks to the elimination of overshoot-correction\njitters while roughly preserving the success rate.\n\nThe third section compares alternatives rewarding schemes\nto the proposed one. In comparison to experiment ID10, ex-\nperiment ID06 superimpose a +1 global goal indicator to the\nproposed reward function and cause both success rate and\nlongitudinal speed metrics to deteriorate. This corroborates\nour previous statement that global goal indicators are redun-\ndant under the presence of a complete set of constraint viola-\ntion penalties. Experiment ID07 and ID08 replaces the pro-\n\n'This happens when ego vehicle drives into dead-end lanes.\n"
    },
    {
      "page_num": 6,
      "figures": [
        {
          "block": {
            "type": "figure",
            "path": "results\\figures\\figure_6.png",
            "y_position": 142,
            "confidence": 0.9915628433227539
          },
          "coords": [
            146,
            142,
            1540,
            635
          ],
          "file_path": "results\\figures\\figure_6.png"
        }
      ],
      "tables": [],
      "text": "Table 2: Experimental results.\n\nposed counter-based penalty with a constant penalty. The re-\nsulting agent behavior is unfavorably sensitive to the penalty\nvalue: a larger value completely bans the access to dangerous\nlanes, resulting in lower longitudinal speed than experiment\nID10; while a smaller value fails to reflect the risk of those\nlanes, resulting in a drastically reduced success rate. Experi-\nment ID09 adds a dense penalty for deviating from the navi-\ngation lane. Although ego vehicle can achieve a higher suc-\ncess rate by sticking to the navigation lane, it can also miss the\nopportunity to temporarily deviate from that lane and over-\ntake slower traffic. The result is significantly lower longitu-\ndinal speed. This exemplifies how dense local-goal related\nrewards can conflict from the global goal.\n\nThe effectiveness of rule-based action masking and non-\nuniform action skipping is illustrated in the fourth table sec-\ntion. We experiment with two set of rules: rule #1 filters out\nlane switching behavior while ego vehicle is moving slowly\non the navigation lane, while rule #2 also banns ego vehicle\nfrom treading onto the opposite lane or biking lane in addi-\ntion. Note rule #2 is stricter than rule #1 but also requires\nmore structural information about the environment. A trade-\noff between safety and efficiency can be identified by com-\nparing Experiment ID11 and ID12: the stricter rule #2 can\nprovide more significant improvements on success rate than\ntule #1, at the price of reduced longitudinal speed. Finally,\nobserve from the last two experiments, non-uniform action\nskipping can further increase the success rate of both masking\ntules by giving the ego car more lane-changing opportunities\nduring inference.\n\n5 Related Work\n\nTemporal abstraction is an effective means for speeding up\nRL algorithms. Although frame skipping with action repe-\n\ntition |Mnih er al., 2013] is a very simple form of temporal\nabstraction, it is extremely effective and has been shown to\n\nbe key to state-of-art performance in many low-level control\ntasks [Braylan et al., 2000]. Due to the discontinuous seman-\n\ntics of tactical driving decisions, we propose to replace ac-\n\ntion repetition with the more stable action skipping method.\nDynamic frame skipping [Lakshminarayanan et al., 2017] is\n\nalso investigated in [Isele et al., 2017] for high-level deci-\n\nsion learning, but it is not as effective as action skipping in\nour experiments. Other more sophisticated form of temporal\nabstractions have also been proposed for RL-based decision\nmaking under the option framework [Sutton et al., 1999], e.g.\nShalev-Shwartz et al., 2016). But they are mostly tailored\nfor specific scenarios and hard to generalize.\n\nAn appropriate multi-dimensional rewarding system is in-\ndispensable for tactical decision making agents that are based\non RL or POMDP. Many existing work applies global goal\n\nindicators, e.g. |Brechtel et al., 2014} |Isele et al., 2017\nMukadam et al., 2017} |Paxton et al., 2017]. However, we\n\nshow that it is unnecessary to use global goal indicators in\ntactical decision making tasks. Constraint violation alerts in\nexisting work are mostly one-shot or constant in time [Brech-|\ntel et al., 2014; |Isele et al., 2017} Paxton et al., 2017\nLi et al., 2017]. We propose to use counter-based risk signals\n\nto speed up learning in multi-lane scenarios. Dense local-\ngoal penalties are also used in some previous work to reg-\n\nulate driving policy heuristically [Ulbrich and Maurer, 2013}\n. We show that such\npenalties can easily mislead the agent towards sub-optimal\npolicies and should be avoided if possible.\n\n6 Conclusion\n\nDeep reinforcement learning is a promising framework to\ntackle the tactical decision making tasks in autonomous driv-\ning systems. In this paper, we propose several practical ingre-\ndients for efficient deep reinforcement learning algorithms to-\nwards tactical decision making. We propose action skipping\nas amore stable alternative for the commonly used action rep-\netition scheme and investigate a necessary set of reward com-\nponents that will guide decision making agent to learn effec-\ntively in complex traffic environments. For more reliable in-\nference, a heuristic rule-based action masker is combine with\nthe learned agent to filters out apparently unsafe actions. The\nproposed ingredients is evaluated in a realistic driving simu-\nlator and results show that they outperform various baseline\nand alternative agent configurations.\n"
    },
    {
      "page_num": 7,
      "figures": [],
      "tables": [],
      "text": "References\n\n[Bojarski et al., 2017] Mariusz Bojarski, Philip Yeres, Anna\nChoromanska, Krzysztof Choromanski, Bernhard Firner,\nLawrence Jackel, and Urs Muller. Explaining how a\ndeep neural network trained with end-to-end learning\nsteers a car. arXiv:1704.07911 [cs], Apr 2017. arXiv:\n1704.07911.\n\n[Braylan er al., 2000] Alex Braylan, Mark Hollenbeck, El-\nliot Meyerson, and Risto Miikkulainen. Frame skip is\na powerful parameter for learning to play atari. Space,\n1600:1800, 2000.\n\n[Brechtel et al., 2014] S. Brechtel, T. Gindele, and R. Dill-\nmann. Probabilistic decision-making under uncertainty\nfor autonomous driving using continuous POMDPs, pages\n\n392-399. Oct 2014.\n\nBrockman et al., 2016] Greg Brockman, Vicki Cheung,\nLudwig Pettersson, Jonas Schneider, John Schulman, Jie\nTang, and Wojciech Zaremba. Openai gym, 2016.\n\nChen et al., 2015] Chenyi Chen, Ari Seff, Alain Korn-\nhauser, and Jianxiong Xiao. DeepDriving: Learning Affor-\ndance for Direct Perception in Autonomous Driving, pages\n2722-2730. 2015.\n\nDonges, 1999] Edmund Donges. A conceptual framework\nfor active safety in road traffic. Vehicle System Dynamics,\n32(2-3):113-128, Aug 1999.\n\nGalceran et al., 2015] Enric Galceran, Alexander G. Cun-\nningham, Ryan M. Eustice, and Edwin Olson. Mul-\ntipolicy Decision-Making for Autonomous Driving via\nChangepoint-based Behavior Prediction. 2015.\n\nIsele et al., 2017] David Isele, Akansel Cosgun, Kaushik\nSubramanian, and Kikuo Fujimura. Navigating intersec-\ntions with autonomous vehicles using deep reinforcement\nlearning. arXiv preprint arXiv:1705.01196, 2017.\n\nLakshminarayanan et al.,2017] Aravind S$. — Lakshmi-\nnarayanan, Sahil Sharma, and Balaraman Ravindran.\nDynamic Action Repetition for Deep Reinforcement\nLearning., pages 2133-2139. 2017.\n\nLi et al., 2017] N. Li, D. W. Oyler, M. Zhang, Y. Yildiz,\nI. Kolmanovsky, and A. R. Girard. Game theoretic mod-\neling of driver and vehicle interactions for verification and\nvalidation of autonomous vehicle control systems. [EEE\nTransactions on Control Systems Technology, PP(99):1—\n16, 2017.\n\n[Miller et al., 2008] Isaac Miller, Mark Campbell, Dan Hut-\ntenlocher, Frank-Robert Kline, Aaron Nathan, Sergei Lu-\npashin, Jason Catlin, Brian Schimpf, Pete Moran, Noah\nZych, and et al. Team cornell’s skynet: Robust perception\nand planning in an urban environment. Journal of Field\nRobotics, 25(8):493-527, Aug 2008.\n\n[Mnih et al., 2013] Volodymyr Mnih, Koray Kavukcuoglu,\nDavid Silver, Alex Graves, Ioannis Antonoglou, Daan\nWierstra, and Martin Riedmiller. Playing atari with deep\nreinforcement learning. arXiv:1312.5602 [cs], Dec 2013.\narXiv: 1312.5602.\n\n[Montemerlo et al., 2008] Michael Montemerlo, Jan Becker,\nSuhrid Bhat, Hendrik Dahlkamp, Dmitri Dolgov, Scott Et-\ntinger, Dirk Haehnel, Tim Hilden, Gabe Hoffmann, and\nBurkhard Huhnke. Junior: The stanford entry in the urban\nchallenge. Journal of field Robotics, 25(9):569-597, 2008.\n\n[Mukadam et al., 2017] Mustafa Mukadam, Akansel Cos-\ngun, Alireza Nakhaei, and Kikuo Fujimura. Tactical de-\ncision making for lane changing with deep reinforcement\nlearning. 2017.\n\n[Paxton et al., 2017] Chris Paxton, Vasumathi Raman, Gre-\ngory D. Hager, and Marin Kobilarov. Combining neural\nnetworks and tree search for task and motion planning in\nchallenging environments. arXiv:1703.07887 [cs], Mar\n2017. arXiv: 1703.07887.\n\nPlessen, 2017] Mogens Graf Plessen. Automating vehicles\nby deep reinforcement learning using task separation with\nhill climbing. arXiv:1711.10785 [cs], Nov 2017. arXiv:\n1711.10785.\n\nSallab et al., 2017] Ahmad El Sallab, Mohammed Abdou,\nEtienne Perot, and Senthil Yogamani. Deep reinforcement\nlearning framework for autonomous driving. Electronic\nImaging, 2017(19):70-76, Jan 2017. arXiv: 1704.02532.\n\nShalev-Shwartz et al., 2016] Shai Shalev-Shwartz,\nShaked Shammah, and Amnon _ Shashua. Safe,\nmulti-agent, reinforcement learning for autonomous\ndriving. arXiv:1610.03295 [cs, stat], Oct 2016. arXiv:\n1610.03295.\n\nShalev-Shwartz et al., 2017] Shai Shalev-Shwartz, Shaked\nShammah, and Amnon Shashua. On a formal model of\nsafe and scalable self-driving cars. arXiv: 1708.06374 [cs,\nstat], Aug 2017. arXiv: 1708.06374.\n\nSutton et al., 1999] Richard S. Sutton, Doina Precup, and\nSatinder Singh. Between mdps and semi-mdps: A frame-\nwork for temporal abstraction in reinforcement learning.\nArtificial Intelligence, 112(1):181-211, Aug 1999.\n\nUlbrich and Maurer, 2013] Simon Ulbrich and Markus\nMaurer. Probabilistic online POMDP decision making\nfor lane changes in fully automated driving, pages 2063-\n2067. IEEE, 2013.\n\nUrmson et al., 2008] Chris Urmson, Joshua Anhalt, Drew\nBagnell, Christopher Baker, Robert Bittner, M. N. Clark,\nJohn Dolan, Dave Duggins, Tugrul Galatali, and Chris\nGeyer. Autonomous driving in urban environments: Boss\nand the urban challenge. Journal of Field Robotics,\n25(8):425-466, 2008.\n\n[van Hasselt et al., 2015] Hado van Hasselt, Arthur Guez,\nand David Silver. Deep reinforcement learning with dou-\nble q-learning. arXiv:1509.06461 [cs], Sep 2015. arXiv:\n1509.06461.\n\n[Wang et al., 2015] Ziyu Wang, Tom Schaul, Matteo Hes-\nsel, Hado van Hasselt, Marc Lanctot, and Nando de Fre-\nitas. Dueling network architectures for deep reinforce-\nment learning. arXiv: 1511.06581 [cs], Nov 2015. arXiv:\n1511.06581.\n\n"
    }
  ]
}