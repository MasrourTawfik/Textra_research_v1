{
  "pdf_name": "1702.02302",
  "pages": [
    {
      "page_num": 1,
      "figures": [
        {
          "block": {
            "type": "figure",
            "path": "results\\figures\\figure_1.png",
            "y_position": 1183,
            "confidence": 0.9948220252990723
          },
          "coords": [
            917,
            1183,
            1481,
            1555
          ],
          "file_path": "results\\figures\\figure_1.png",
          "description": "The figure presents a schematic representation of an autonomous braking system using deep reinforcement learning. It is related to the text passage as it visually summarizes the components and interactions within the system. The figure is divided into two main sections: 'Agent' and 'Environment'.\n\n\nThe 'Agent' section includes a 'Brake control' component, which is symbolized by a brake pedal icon, and a 'State Reward' component, which is represented by a dollar sign. These elements are connected by arrows indicating the flow of information and actions. The 'Agent' interacts with the 'Environment', which is depicted with a car and a pedestrian, symbolizing the driving scenario. The 'Environment' is also connected to a 'Sensor fusion' component, which is represented by a gear icon, indicating the integration of sensor data.\n\n\nThe main elements and structure of the figure include the 'Agent' and 'Environment', the 'Brake controller', the 'Sensor fusion', and the 'Drive' component within the 'Environment'. The key metrics or patterns shown are the flow of actions and rewards between the 'Agent' and the 'Environment', which is indicative of the reinforcement learning process. The mathematical or algorithmic aspects are not explicitly detailed in the figure, but the concept of state rewards and actions suggests a feedback loop where the agent learns to optimize braking based on the environment's response.\n\n\nThe research impact of this system is significant as it contributes to the development of autonomous vehicles by providing a framework for implementing deep reinforcement learning in real-world driving scenarios. The figure illustrates the practical application of the proposed system, showing how it can potentially improve safety and efficiency in autonomous driving.",
          "context_found": true,
          "reference_text": "1702.02302v2 [cs.AI] 24 Apr 2017\n\niv\n\narxX\n\nAutonomous Braking System via\nDeep Reinforcement Learning\n\nHyunmin Chae, Chang Mook Kang, ByeoungDo Kim, Jaekyum Kim, Chung Choo Chung, and Jun Won Choi\nHanyang University, Seoul, Korea\nEmail: hmchae@spo.hanyang.ac.kr, kem0728 @hanyang.ac.kr, {bdkim, jkkim} @spo.hanyang.ac.kr,\n{cchung ,junwchoi} @hanyang.ac.kr\n\nAbstract—In this paper, we propose a new autonomous braking\nsystem based on deep reinforcement learning. The proposed\nautonomous braking system..."
        }
      ],
      "tables": [],
      "text": "1702.02302v2 [cs.AI] 24 Apr 2017\n\niv\n\narxX\n\nAutonomous Braking System via\nDeep Reinforcement Learning\n\nHyunmin Chae, Chang Mook Kang, ByeoungDo Kim, Jaekyum Kim, Chung Choo Chung, and Jun Won Choi\nHanyang University, Seoul, Korea\nEmail: hmchae@spo.hanyang.ac.kr, kem0728 @hanyang.ac.kr, {bdkim, jkkim} @spo.hanyang.ac.kr,\n{cchung ,junwchoi} @hanyang.ac.kr\n\nAbstract—In this paper, we propose a new autonomous braking\nsystem based on deep reinforcement learning. The proposed\nautonomous braking system automatically decides whether to\napply the brake at each time step when confronting the risk of\ncollision using the information on the obstacle obtained by the\nsensors. The problem of designing brake control is formulated\nas searching for the optimal policy in Markov decision process\n(MDP) model where the state is given by the relative position\nof the obstacle and the vehicle’s speed, and the action space is\ndefined as the set of the brake actions including 1) no braking,\n2) weak, 3) mid, 4) strong brakiong actions. The policy used\nfor brake control is learned through computer simulations using\nthe deep reinforcement learning method called deep Q-network\n(DQN). In order to derive desirable braking policy, we propose\nthe reward function which balances the damage imposed to the\nobstacle in case of accident and the reward achieved when the\nvehicle runs out of risk as soon as possible. DQN is trained for\nthe scenario where a vehicle is encountered with a pedestrian\ncrossing the urban road. Experiments show that the control agent\nexhibits desirable control behavior and avoids collision without\nany mistake in various uncertain environments.\n\nI. INTRODUCTION\n\nSafety is one of top priorities that should be pursued in\nrealizing fully autonomous driving vehicles. For safe au-\ntonomous driving, autonomous vehicles should perceive the\nenvironments using the sensors and control the vehicle to travel\nto the destination without any accidents. Since it is inevitable\nfor an autonomous vehicle to encounter with unexpected and\nrisky situations, it is critical to develop the reliable autonomous\ncontrol systems that can cope well with such uncertainties.\nRecently, several safety systems including collision avoidance,\npedestrian detection, and front collision warning (FCW) have\nbeen proposed to enhance the safety of the autonomous vehicle\n\nOne critical component for enabling safe autonomous driv-\ning is the autonomous braking systems which can reduce\nthe velocity of the vehicle automatically when a threatening\nobstacle is detected. The autonomous braking should offer safe\nand comfortable brake control without exhibiting too early\nor too late braking. Most conventional autonomous braking\nsystems are rule-based, which designate the specific brake\ncontrol protocol for each different situation. Unfortunately, this\napproach is limited in handling all scenarios that can happen\nin real roads. Hence, the intelligent braking system should\nbe developed to avoid the accidents in a principled and goal-\noriented manner.\n\nRecently, interest in machine learning has explosively grown\nup with the rise of parallel computing technology and a\nlarge amount of training data. In particular, the success of\ndeep neural network (DNN) technique led the researchers to\ninvestigate the application of machine learning for autonomous\ndriving. The DNN has been applied to autonomous driving\nfrom camera-based perception [4]-[6] to end-to-end approach\nwhich learns mapping from the sensing to the control [7],\n[8]. Reinforcement learning (RL) technique has also been\nimproved significantly as DNN was adopted. The technique,\ncalled deep reinforcement learning (DRL), has shown to\nperform reasonably well for various challenging robotics and\ncontrol problems. In [9], the DRL technique called Deep Q-\nnetwork (DQN) was proposed, which approximates Q-value\nfunction using DNN. It was shown that the DQN can outper-\nform human experts in various Atari video games. Recently,\nthe DRL is applied to control systems for autonomous driving\nvehicle in\n\nFig. 1. | The proposed DRL-based autonomous braking systems.\n\nIn this paper, we propose a new autonomous braking system\nbased DRL, which can intelligently control the velocity of the\nvehicle in situations where collision is expected if no action is\ntaken. The proposed autonomous braking system is described\nin Fig. The agent (vehicle) interacts with the uncertain\nenvironment where the position of the obstacle could change\nin time and thus the risk of collision at each time step varies\nas well. The agent receives the information of the obstacle’s\nposition using the sensors and adapts the brake control to the\nstate change such that the chance of accident is minimized.\n\nIn our work, we design the autonomous braking system for\nthe urban road scenario where a vehicle faces a pedestrian\n"
    },
    {
      "page_num": 2,
      "figures": [
        {
          "block": {
            "type": "figure",
            "path": "results\\figures\\figure_2.png",
            "y_position": 1347,
            "confidence": 0.993439257144928
          },
          "coords": [
            227,
            1347,
            769,
            1890
          ],
          "file_path": "results\\figures\\figure_2.png",
          "description": "Based on the figure and the surrounding text, the analysis is as follows:\n\n1. Contextual Placement:\nThe figure appears to be a diagram illustrating the relationships between different states in a reinforcement learning model, specifically in the context of an autonomous braking system. It shows three states: 'Sstay', 'Scross', and 'Snobody', with arrows indicating the transition probabilities between these states. This diagram is likely related to the section discussing the proposed autonomous braking system based on deep reinforcement learning, as it visually represents the state transitions which are a fundamental aspect of reinforcement learning algorithms.\n\n2. Technical Analysis:\nThe main elements of the diagram are the states 'Sstay', 'Scross', and 'Snobody', which represent different conditions or scenarios in the autonomous braking system. The arrows between the states indicate the transition probabilities, which are denoted by 'p(stay|stay)', 'p(cross|stay)', 'p(stay|cross)', and 'p(cross|cross)'. These probabilities are crucial for understanding the dynamics of the system and how it learns to make decisions based on the current state and the outcome of previous actions. The diagram is a simplified representation of the state transition model used in the reinforcement learning algorithm for the autonomous braking system.\n\n3. Research Impact:\nThe diagram likely illustrates the key metric of transition probabilities, which are essential for the reinforcement learning algorithm to learn the optimal policy for the autonomous braking system. By showing the probabilities of staying in the current state or crossing to a different state, the diagram helps to visualize the learning process and the decision-making framework of the system. The research contributes to the methodology by providing a clear representation of how the system learns and adapts over time, which is a significant aspect of deep reinforcement learning.",
          "context_found": true,
          "reference_text": "1702.02302v2 [cs.AI] 24 Apr 2017\n\niv\n\narxX\n\nAutonomous Braking System via\nDeep Reinforcement Learning\n\nHyunmin Chae, Chang Mook Kang, ByeoungDo Kim, Jaekyum Kim, Chung Choo Chung, and Jun Won Choi\nHanyang University, Seoul, Korea\nEmail: hmchae@spo.hanyang.ac.kr, kem0728 @hanyang.ac.kr, {bdkim, jkkim} @spo.hanyang.ac.kr,\n{cchung ,junwchoi} @hanyang.ac.kr\n\nAbstract—In this paper, we propose a new autonomous braking\nsystem based on deep reinforcement learning. The proposed\nautonomous braking system..."
        },
        {
          "block": {
            "type": "figure",
            "path": "results\\figures\\figure_2.png",
            "y_position": 972,
            "confidence": 0.9830613732337952
          },
          "coords": [
            955,
            972,
            1462,
            1248
          ],
          "file_path": "results\\figures\\figure_2.png",
          "description": "Based on the figure and the surrounding text, the analysis is as follows:\n\n1. Contextual Placement:\nThe figure appears to be a diagram illustrating the relationships between different states in a reinforcement learning model, specifically in the context of an autonomous braking system. It shows three states: 'Sstay', 'Scross', and 'Snobody', with arrows indicating the transition probabilities between these states. This diagram is likely related to the methodology or results section of the paper, where the authors describe the state transitions and the reinforcement learning algorithm used to achieve autonomous braking.\n\n2. Technical Analysis:\nThe main elements of the diagram are the states 'Sstay', 'Scross', and 'Snobody', which represent different conditions or scenarios in the autonomous braking system. The arrows between the states indicate the transition probabilities, which are labeled with 'p(stay|stay)', 'p(cross|stay)', 'p(stay|cross)', and 'p(cross|cross)'. These probabilities likely represent the likelihood of transitioning from one state to another based on the current state and the action taken. The diagram is a visual representation of the state transition model used in the reinforcement learning algorithm for the autonomous braking system.\n\n3. Research Impact:\nThe diagram and the surrounding text suggest that the authors have developed a new autonomous braking system using deep reinforcement learning. The state transition model is a key component of this system, and the diagram provides a visual representation of how the system operates. The research impact is likely to be in the field of autonomous vehicles or robotics, where such systems are crucial for safety and efficiency. The proposed methodology and results could contribute to the development of more advanced and reliable autonomous braking systems.",
          "context_found": true,
          "reference_text": "1702.02302v2 [cs.AI] 24 Apr 2017\n\niv\n\narxX\n\nAutonomous Braking System via\nDeep Reinforcement Learning\n\nHyunmin Chae, Chang Mook Kang, ByeoungDo Kim, Jaekyum Kim, Chung Choo Chung, and Jun Won Choi\nHanyang University, Seoul, Korea\nEmail: hmchae@spo.hanyang.ac.kr, kem0728 @hanyang.ac.kr, {bdkim, jkkim} @spo.hanyang.ac.kr,\n{cchung ,junwchoi} @hanyang.ac.kr\n\nAbstract—In this paper, we propose a new autonomous braking\nsystem based on deep reinforcement learning. The proposed\nautonomous braking system..."
        }
      ],
      "tables": [],
      "text": "who crosses the street at a random timing. In order to find\nthe desirable brake action for the given pedestrian’s location\nand vehicle’s speed, we need to allocate appropriate reward\nfunction for each state-action pair. In our work, we focus on\nfinding the desirable reward function which strikes the balance\nbetween the penalty imposed to the agent when accident\nhappens and the reward obtained when the vehicle quickly gets\nout of risk. Using the reward function we carefully designed,\nwe train DQN to learn the policy that decides the timing of\nbrake based on the given pedestrian’s state. We also provide a\nnew DQN design which can rapidly learn the policy to avoid\nrare accidents.\n\nVia computer simulations, we evaluate the performance of\nthe proposed autonomous braking system. In simulations, we\nconsider the uncertainty of the vehicle’s initial velocity, pedes-\ntrian’s initial position, and whether the pedestrian will cross or\nnot. The experimental results show that the proposed braking\nsystem exhibits desirable control behavior for various test\nscenarios including autonomous emergency braking (AEB)\ntest administrated by Euro NCAP.\n\nThe rest of this paper is organized as follows. In Section\nII, we describe the basic scenarios and the framework of the\nproposed system. In Section III, we provide the details of\nthe DQN design for autonomous braking. The experimental\nresults are provided in Section IV and the paper is concluded\nin Section V.\n\nIl. SYSTEM DESCRIPTION\n\nIn this section, we describe the overall structure of the\nautonomous braking system. We first define the possible\nscenarios for autonomous braking and explain the detailed\noperation of the proposed system.\n\nFig. 2. Though a pedestrian is detected for the cases (a) and (b), the proper\ncontrol actions are different for two cases. For case (a), the vehicle should\nstop in front of the pedestrian while for case (b), the vehicle should be on\nstandby without stepping the brake yet.\n\nA. Scenarios\n\nOne of the factors that hinders safe driving in autonomous\ndriving is the threat from nearby objects, e.g. pedestrians.\nMany accidents could happen when the vehicle fails to stop\nahead of it when a pedestrian crosses the road. Hence, in order\nto avoid accidents, the vehicle should detect the threat that can\npotentially cause accidents in advance and perform appropriate\nbrake actions to stop vehicle in front of the obstacle. However,\nthere exist various degrees of uncertainty which make the\ndesign of autonomous braking challenging such as\n\ne Vehicle’s initial velocity\n\ne Pedestrian’s position\n\ne Pedestrian’s speed\n\ne Pedestrian’s crossing timing\ne Pedestrian’s moving direction\ne Sensor’s measurement error\ne Road’s condition\n\nEven if a pedestrian is detected accurately, it is hard to\nknow when it can become a threat to the vehicle. Hence\nwe need appropriate braking strategy for different situations.\n(see Fig. 2]) That is, for the given state of the pedestrian\n(i.e. position, velocity), the autonomous braking system should\ndecide what brake action to apply.\n\nFig. 3.\nprocess.\n\nBehavior of a pedestrian modeled by the discrete-state Markov\n\nIn our system, we consider the scenario where behavior\nof the pedestrian follows the discrete-state Markov process\ndescribed in Fig. [3] The state S;,o).ay implies that the sensors\nhave not detected any obstacle. Once a pedestrian is detected,\nthe state Spobody can change to the state Ssiay or the state\nScross, Where Stay is the state that the pedestrian stays at\nsidewalk and S.;oss is the state that the pedestrian crosses the\nroad. The pedestrian’s initial position can be either from far-\nside and near-side of the vehicle and the pedestrian walking\nspeed can vary between Upeq\"’” m/s and Uped’™™** m/s.\nNote that the vehicle’s initial velocity is distributed between\nVen” m/s and Vyen™\"* m/s. In practical scenarios, it is\ndifficult to know the transition probabilities of the Markov pro-\ncess and the distribution of the pedestrian’s states. Therefore,\nreinforcement learning approach can be applied to learn the\nbrake control policy through the interaction with environment.\n\nB. Autonomous Braking System\n\nThe detailed operation of the proposed autonomous brak-\ning system is depicted in Fig. The vehicle is moving\n"
    },
    {
      "page_num": 3,
      "figures": [
        {
          "block": {
            "type": "figure",
            "path": "results\\figures\\figure_3.png",
            "y_position": 1677,
            "confidence": 0.9697415828704834
          },
          "coords": [
            154,
            1677,
            821,
            1976
          ],
          "file_path": "results\\figures\\figure_3.png",
          "description": "1. Contextual Placement:\nThe figure is a visual representation of a scenario where a vehicle and a pedestrian interact at a pedestrian crossing. It is placed in the context of the text passage, which discusses the importance of designing a reward function for a reinforcement learning model to determine the appropriate braking action for a vehicle when a pedestrian crosses the street. The figure serves as a visual aid to help understand the dynamics of the interaction between the vehicle and pedestrian, which is crucial for the context of the text.\n\n2. Technical Analysis:\nThe main elements of the figure include the vehicle, pedestrian, pedestrian crossing trigger point, vehicle position, vehicle velocity, and time interval. The vehicle's position and velocity are shown before and after the pedestrian crosses the trigger point, indicating the vehicle's reaction to the pedestrian's presence. The time interval is represented by a horizontal dashed line, with the action taken by the vehicle (acceleration) shown as a vertical dashed line. The reward function is not explicitly shown, but it is implied that it would be based on the vehicle's response to the pedestrian's crossing, balancing the penalty for accidents and the reward for quick reaction.\n\nKey metrics or patterns shown include the vehicle's velocity before and after the pedestrian crosses, the trigger point, and the stop line. The mathematical or algorithmic aspects are not directly shown, but the figure implies the need for a reward function that considers the time interval (ΔT) and the action taken by the vehicle (A_t) to determine the desirable braking action.\n\n3. Research Impact:\nThe figure illustrates the critical moment of a pedestrian crossing and the vehicle's response, which is central to the research's objective of finding a reward function that balances the risk of accidents with the efficiency of the vehicle's reaction. The research aims to train a Deep Q-Network (DQN) to learn the optimal braking action based on the vehicle's speed and the pedestrian's position. The figure helps to visualize the scenario and the factors that need to be considered when designing the reward function, which is a key contribution to the methodology and results of the research.",
          "context_found": true,
          "reference_text": "who crosses the street at a random timing. In order to find\nthe desirable brake action for the given pedestrian’s location\nand vehicle’s speed, we need to allocate appropriate reward\nfunction for each state-action pair. In our work, we focus on\nfinding the desirable reward function which strikes the balance\nbetween the penalty imposed to the agent when accident\nhappens and the reward obtained when the vehicle quickly gets\nout of risk. Using the reward function we carefully designed,\nwe train DQN..."
        }
      ],
      "tables": [],
      "text": "at speed Uyen from the position (vehpos,, vehposy). As\nsoon as a pedestrian is detected, the autonomous braking\nsystem receives the relative position of the pedestrian, i.e.,\n(pedpos, — vehpos.,pedposy — vehposy,) from the sensor\nmeasurements where (pedpos,, pedposy) is the location of the\npedestrian. Using the vehicle’s velocity vyen and the relative\nposition (pedpos, —vehpos,, pedposy —vehpos,), the vehicle\ndecides whether it will step brake at each time step. The\ninterval between consecutive time steps is given by AT. We\nconsider four brake actions; no braking dnorning and braking\nGhighs Amid aNd Qjow With different intensities. We can include\nmore brake actions with more refined steps or continuous\nbrake action which are not considered in this work.\n\nII. DEEP REINFORCEMENT LEARNING FOR\nAUTONOMOUS BRAKING SYSTEM\n\nIn this section, we present the details of the proposed\nDRL-based autonomous braking system. We first introduce the\nstructure of the DQN and explain the reward function used to\ntrain the DQN in details.\n\nA. Structure of DRL\n\nOur system follows the basic RL structure. The agent\nperforms an action A; given state 5, under policy 7. The\nagent receives the state as feedback from the environment and\ngets the reward r, for the action taken. The state feedback\nthat the agent takes from sensors consists of the velocity of\nthe vehicle v,-, and the relative position to the pedestrian,\n(pedpos, — vehpos,, pedposy — vehposy) for the past n time\nsteps. Possible action that agent can choose is among decelera-\ntion Gnighs Amid; Glow and keeping the current speed Gnotning-\nThe goal of our proposed autonomous braking system is\nto maximize the expected accumulated reward called “value\nfunction” that will be received in the future within an episode.\nUsing the simulations, the agent learns from interaction with\nenvironment episode-by-episode. One episode starts when a\npedestrian is detected. Note that the initial position of the\npedestrian and the initial velocity of the vehicle are random.\nThe vehicle drives on a straight way based on the brake policy\nm. If the distance between the vehicle and the pedestrian is less\nthan the safety distance 1, it is considered as a collision event.\n(see Fig. [4]) The episode ends if at least one of the following\nevents occurs\n\ne Stop : the vehicle completely stops, i.e., Vsen = 0.\n\nFig. 4. Illustration of the autonomous braking operation\n\n« Bump : the vehicle passes the safety line | when the\npedestrian is crossing road.\ne Pass : the vehicle passes the pedestrian without accident.\n¢ Cross : the pedestrian completely crosses the road and\nreaches the opposite side.\nOnce one episode ends, the next episode starts with the state\nof environment and the value function reset.\n\nB. Deep Q-Network\n\nQ-learning is one of the popular RL methods which searches\nfor the optimal policy in an iterative fashion [12]. Basically,\nthe Q-value function g,(s,a) is defined as\n\ndn (8,0) = Ex [Dg y*re4n41 [St = 8, Ar = a] (1)\n\nfor the given state s and action a, where r; is the reward\nreceived at the time step t. The Q-value function is the\nexpected sum of the future rewards which indicates how good\nthe action a is given the state s under the policy of the agent 7.\nThe contribution to the Q-value function decays exponentially\nwith the discounting factor y for the rewards with far-off\nfuture. For the given Q-value function, the greedy policy is\nobtained as\n\nm(s) = arg max q;(s, a). (2)\n\nOne can show that for the policy in (2), the following Bellman\nequation should hold [12]\n\ndn(5,0) =E [regs + ymax de(Se41,4\")|S¢ = 8, Ae = al -\n(3)\nIn practice, since it is hard to obtain the exact value of q,(s, a)\nsatisfying the Bellman equation, the Q-learning method uses\nthe following update rule for the given one step backups S;,\nAp, Trt, $1413\nIn (St, Ae) — Gn (St, At)\n+a (Ga + ymax dr(Si41, a) — qr (Si, A,))\n(4)\nHowever, when the state space is continuous, it is impossible\nto find the optimal value of the state-action pair q.(s, a)\nfor all possible states. To deal with this problem, the DQN\nmethod was proposed, which approximates the state-action\nvalue function g(s,a) using the DNN, ie., q(s,a) © go(s, a)\nwhere @ is the parameter of the DNN [9]. The parameter @ of\n\nthe DNN is then optimized to minimize the squared value of\nthe temporal difference error 5;\n\nJe = Pega + ymax go(Si41,4) — go(Se, Ar) (5)\n\nFor better convergence of the DQN, instead of estimating both\nq(St, At) and q(St41,@’) in &), we approximate q(5;, Ar)\nand q(S:41,a’) using the Q-network and the target network\nparameterized by 6 and 0~, respectively [9]. The update of the\ntarget network parameter 6~ is done by cloning Q-network\nparameter 0, periodically. Thus, (5) becomes\n\nJe = Pega + ymax dqo- (S414) — go(Se, Ar) (6)\n"
    },
    {
      "page_num": 4,
      "figures": [],
      "tables": [],
      "text": "To speed up convergence further, replay memory is adopted\nto store a bunch of one step backups and use a part of them\nchosen randomly from the memory by batch size [9]. The\nbackups in the batch is used to calculate the loss function L\nwhich is given by\n\nL=Xitew, On, (7)\n\nreplay\n\nwhere Byeplay is the backups in the batch selected from\nreplay memory. Note that the optimization of parameter @ for\nminimizing the loss L is done through the stochastic gradient\ndecent method.\n\nC. Reward Function\n\nUnlike video games, the reward should be appropriately\ndefined by a system designer in autonomous braking system.\nAs mentioned, the reward function determines the behavior\nof the brake control. Hence, in order to ensure the reliability\nof the brake control, it is crucial to use the properly defined\nreward function. In our model, there is conflict between two\nintuitive objectives for brake control; 1) collision should be\navoided no matter what happens and 2) the vehicle should get\nout of the risky situation quickly. If it is unbalanced, the agent\nbecomes either too conservative or reckless. Therefore, we\nshould use the reward function which balances two conflicting\nobjectives. Taking this into consideration, we propose the\nfollowing reward function\n\nr, = —(a(pedpos,—vehpos,)” + B)decel\n— (ner? + A)U(St = bump) (8)\na, 8,7,A >0\n\nwhere v; is the velocity of the vehicle at the time step t,\ndecel is difference between v, and v,_; and I(x = y) has\na value of 1 if the statement inside is true and 0 otherwise.\nThe first term —(a(pedpos, — vehpos,)? + 8)decel in the\nreward function prevents the agent from braking too early by\ngiving penalty proportional to squared distance between the\nvehicle and pedestrian. It guides the vehicle to drive without\ndeceleration if the pedestrian is far from the vehicle. On the\nother hand, the term —(7v,? + A)1(S; = bump) indicates the\npenalty that the agent receives when the accident occurs. Note\nthat this penalty is a function of the vehicle’s velocity, which\nreflects the severe damage to the pedestrian in case of high\nvelocity at collision. Without such dependency on the velocity,\nthe agent would not reduce the speed in situation when the\naccident is not avoidable. The constants a, 3, 7 and X are\nthe weight parameters that controls the trade-off between two\nobjectives.\n\nD. Trauma Memory\n\nAs mentioned in the previous section, autonomous braking\nsystems should learn both of the conflicting objectives. How-\never, when we train the DQN with the reward function in\n(8), we find that the learning performance is not stable since\ncollision events rarely happen and thus there remains only\n\na few one-step backups associated with the collisions in the\nreplay memory. As a result, the probability of picking such\none-step backups is small and the DQN does not have enough\nchance to learn to avoid accidents in practical learning stage.\nTo solve this issue, we propose so called trauma” memory\nwhich is used to store only the one-step backups for the rare\nevents (e.g., collision events in our scenario). While the one\nstep backups are randomly picked from the replay memory,\nsome fixed number of backups associated with the collision\nevents are randomly selected from the trauma memory and\nused for training together. In other words, with the trauma\nmemory, the loss function L is modified to\n\nL=%Xtep 57 + DeeBeraumna St” (9)\n\nwhere Biyauma is the backups randomly picked from trauma\nmemory. Trauma memory persistently reminds the agent of\nthe memory on the accidents regardless of the current policy,\nthus allowing the agent to learn to maintain speed and avoid\ncollisions reliably.\n\nreplay\n\nIV. EXPERIMENTS\n\nIn this section, we evaluate the performance of the proposed\nautonomous braking system via computer simulations.\n\nA. Simulation Setup\n\nIn simulations, we used the commercial software PreScan\nwhich models vehicle dynamics in real time [15]. We gener-\nated the environment in order to train the DQN by simulating\nthe random behavior of the pedestrian. In the simulations,\nwe assume that the relative location of the pedestrian is\nprovided to the agent. To make the system practical, we add\nslight measurement noise to it. In each episode, the initial\nposition of vehicle is set to (0,0). Time-to-collision TTC is\nchosen according to the uniform distrubution between 1.5 s\nand 4 s. The initial velocity of the vehicle is uniformly\ndistributed between vinit”” = 2.78 m/s (10 km/s) and\nVinit”™“” = 16.67 m/s (60 km/h). At the beginning of the\nepisodes, the position of the pedestrian is fixed to 5 * Uinit\nmeters away from the position of the vehicle. The pedestrian\nstands either at the far-side or at near-side of the vehicle with\nequal probability. The behavior of the pedestrian follows one\nof two scenarios below;\n\ne Scenario | : Cross the road\n\ne Scenario 2 : Stay at initial position.\n\nDuring training, either of two scenarios is selected with equal\nprobability. In Scenario 1, the pedestrian starts to move when\nthe vehicle is crossing at the “pedestrian crossing point”\nPtrig = (5 — TTC) * vinit. (See Fig. AJ) The safety distance 1\nfor the pedestrian is set to 3 m. The agent chooses the brake\ncontrol among anigh = —9.8 m/s?, @mia = —5.9 m/s?,\nGhigh = —2.9 m/s? and Gnothing = 0 m/s? every AT = 0.1\nsecond. The detailed simulation setup is summarized below.\n\ne Initial velocity of vehicle viniy ~ U(2.78, 16.67) m/s\n\n¢ Velocity of pedestrian Upeq ~ U(2,4) m/s\n\ne Time-to-collision TTC ~ U(1.5,4) s\n\ne Initial pedestrian position pedpos, = 5 * Vinit M\n"
    },
    {
      "page_num": 5,
      "figures": [
        {
          "block": {
            "type": "figure",
            "path": "results\\figures\\figure_5.png",
            "y_position": 168,
            "confidence": 0.9952929615974426
          },
          "coords": [
            901,
            168,
            1532,
            777
          ],
          "file_path": "results\\figures\\figure_5.png",
          "description": "Based on the figure and the surrounding text, the analysis is as follows:\n\n1. Contextual Placement:\nThe figure provides a visual representation of the longitudinal position of a vehicle and a pedestrian, the velocity of the vehicle, and the action trajectory over time. This visual data is crucial for understanding the dynamics of the system being analyzed, which is likely related to a vehicle-pedestrian interaction scenario. The figure complements the text passage by offering a graphical depiction of the concepts discussed, such as the velocity of the vehicle and the action trajectory, which are essential for understanding the context of the replay memory and optimization process mentioned in the text.\n\n2. Technical Analysis:\nThe figure consists of three graphs:\n\n- The first graph shows the longitudinal position of the vehicle and pedestrian over time, with the vehicle's position represented by a red line and the pedestrian's position by a blue line. This graph indicates that the vehicle's position remains constant while the pedestrian's position increases linearly over time.\n\n- The second graph displays the velocity of the vehicle over time, with the velocity represented by blue circles. The velocity starts at a higher value and gradually decreases, suggesting that the vehicle is slowing down over time.\n\n- The third graph illustrates the action trajectory, which is a bar graph showing the deceleration of the vehicle over time. The bars increase in height from 0 to 6 seconds, indicating that the vehicle's deceleration increases as time progresses.\n\nThe figure's structure, with its clear labels and distinct colors, allows for easy interpretation of the data. The key metrics shown are the position, velocity, and deceleration of the vehicle, which are essential for understanding the system's behavior.\n\n3. Research Impact:\nThe figure, along with the text, suggests that the research involves a method for speeding up the convergence of a learning process, possibly in a reinforcement learning context. The use of replay memory and the optimization of parameters through stochastic gradient descent are indicative of advanced techniques in machine learning. The visual data provided by the figure helps to illustrate the effectiveness of these methods in achieving the desired outcome, which is likely to be a more efficient and accurate learning process. The research contributes to the field by demonstrating the practical application of these techniques in real-world scenarios,",
          "context_found": true,
          "reference_text": "To speed up convergence further, replay memory is adopted\nto store a bunch of one step backups and use a part of them\nchosen randomly from the memory by batch size [9]. The\nbackups in the batch is used to calculate the loss function L\nwhich is given by\n\nL=Xitew, On, (7)\n\nreplay\n\nwhere Byeplay is the backups in the batch selected from\nreplay memory. Note that the optimization of parameter @ for\nminimizing the loss L is done through the stochastic gradient\ndecent method.\n\nC. Reward Function\n\nUnlik..."
        },
        {
          "block": {
            "type": "figure",
            "path": "results\\figures\\figure_5.png",
            "y_position": 1052,
            "confidence": 0.992642343044281
          },
          "coords": [
            869,
            1052,
            1558,
            1626
          ],
          "file_path": "results\\figures\\figure_5.png",
          "description": "The figure presents three separate graphs that illustrate the longitudinal position of a vehicle and pedestrian, the velocity of the vehicle, and the action trajectory over time.\n\n1. **Longitudinal Position of Vehicle and Pedestrian**:\n   - The top graph shows the longitudinal position of both the vehicle and pedestrian over a period of 6 seconds.\n   - The vehicle's position is represented by a red line with crosses, which remains constant at 80 meters, indicating that the vehicle is stationary.\n   - The pedestrian's position is represented by a blue line with circles, which increases linearly from 0 to 60 meters, suggesting that the pedestrian is moving forward at a constant speed.\n\n2. **Velocity of Vehicle**:\n   - The middle graph depicts the velocity of the vehicle over the same 6-second period.\n   - The vehicle's velocity starts at 20 m/s and decreases linearly to 0 m/s, indicating a deceleration.\n\n3. **Action Trajectory**:\n   - The bottom graph shows the deceleration of the vehicle over time.\n   - The deceleration starts at 8 m/s² and increases to 10 m/s², with a step change at 4 seconds, where the deceleration jumps to 15 m/s².\n   - The bar graph format emphasizes the change in deceleration at the 4-second mark.\n\nThe surrounding text discusses the use of replay memory to store one-step backups and the use of a batch size to randomly select backups for calculating the loss function. This suggests that the figure may be related to a machine learning model, possibly a reinforcement learning model, where the action trajectory and deceleration are part of the training process. The context implies that the model is being trained to optimize parameters to minimize the loss function, which is likely related to the vehicle's ability to safely decelerate and stop.\n\nThe figure's role in the current section is to provide a visual representation of the vehicle's and pedestrian's movements and the vehicle's deceleration, which are critical for understanding the dynamics of the system being modeled.\n\nThe main elements and structure of the figure include the three graphs,",
          "context_found": true,
          "reference_text": "To speed up convergence further, replay memory is adopted\nto store a bunch of one step backups and use a part of them\nchosen randomly from the memory by batch size [9]. The\nbackups in the batch is used to calculate the loss function L\nwhich is given by\n\nL=Xitew, On, (7)\n\nreplay\n\nwhere Byeplay is the backups in the batch selected from\nreplay memory. Note that the optimization of parameter @ for\nminimizing the loss L is done through the stochastic gradient\ndecent method.\n\nC. Reward Function\n\nUnlik..."
        }
      ],
      "tables": [],
      "text": "¢ Trigger point prrig = (5 — TTC) * vinis m\ne Safety line 1 = 3 m\ne AT=0.15\n\n© Ghigh; Imid; Glow, Anothing =\n\n{—9.8, —5.9, —2.9, 0} m/s?\n\nB. Training of DON\nThe neural network used for the DQN consists of the fully-\nconnected layers with five hidden layers. RMSProp algorithm\nis used to minimize the loss with learning rate 4 =\n0.0005. The number of position data samples used as a state is\nset to n = 5. We set the size of the replay memory to 10,000\nand that of the trauma memory to 1,000. We set the replay\nbatch size to 32 and trauma batch size to 10. The summary of\nthe DQN configurations used for our experiments is provided\nbelow;\n¢ State buffer size: n = 5\ne Network architecture: fully-connected feedforwared net-\nwork\ne Nonlinear function: leaky ReLU\ne Number of nodes for each layers : [15(Input layer), 100,\n70, 50, 70, 100, 4(Output layer)]\ne¢ RMSProp optimizer with learning rate 0.0005\ne Replay memory size: 10,000\ne Replay batch size: 32\ne Trauma memory size: 1,000\n¢ Trauma batch size = 10\ne Reward function: a = 0.001, 6\n100\nFig. provides the plot of the total accumulated rewards\ni.e., value function achieved for each episode when training\nis conducted with and without trauma memory. We observe\nthat with trauma memory the value function converges after\n2,000 episodes and high total reward is steadily attained after\nconvergence while without trauma memory the policy does\nnot converge and keeps fluctuating.\n\n0.1, 7 = 0.01,\n\nC. Test Results\n\nSafety test was conducted for several different TTC values.\nCollision rate is measured for 10,000 trials for each TTC value.\nTable I provides the collision rate for each TTC value for\nthe test performed for Scenario 1. The agent avoids collision\nsuccessfully for TTC values above 1.5s. For the cases with\nTTC values less than 1.5s, we observe some collisions.\nAccording to our analysis on the trajectory of braking actions,\nthese are the cases where collision was not avoidable due to\nthe high initial velocity of the vehicle even though full braking\nactions were applied. The agent passed the pedestrian without\nunnecessary stop for all cases in the Scenario 2. The detailed\ntrajectory of the brake actions for one example case is shown\nin Fig. [6] Fig. [6](a) shows the trajectory of the position of the\nvehicle and the pedestrian recorded every 0.1 s. The velocity\nof the agent and the brake actions applied are shown in Fig. [6]\n(b) and (c), respectively. The vehicle starts to decelerate about\n20 m away from the pedestrian and completely stops about 5 m\nahead, thereby accomplishing collision avoidance. We observe\n\nFig. 5. Achieved value function achieved during training\n\nthat weak braking actions are applied in the beginning part of\ndeceleration and then strong braking actions come as the agent\ngets close to the pedestrian.\n\nFig. 6. Trajectory of position, velocity and actions in one example episode\nfor the case TTC = 1.58\n\nFig. shows how the initial position of the pedestrian\nand the relative distance between the pedestrian and vehicle\nare distributed for 1,000 trials in the scenario 1. We see\nthat the vehicle stops around 5 m in front of the pedestrian\nfor most of cases. This seems to be reasonable safe braking\noperation considering the safety distance of 1 = 3 m. Note\nthat this distance can be adjusted by changing the reward\nparameters. Overall, the experimental results show that the\n"
    },
    {
      "page_num": 6,
      "figures": [
        {
          "block": {
            "type": "figure",
            "path": "results\\figures\\figure_6.png",
            "y_position": 482,
            "confidence": 0.9956225752830505
          },
          "coords": [
            199,
            482,
            770,
            1001
          ],
          "file_path": "results\\figures\\figure_6.png",
          "description": "The figure is a scatter plot with two sets of data points, one represented by blue circles and the other by red crosses. The x-axis is labeled \"Episode\" and ranges from 0 to 1000, while the y-axis is labeled \"Distance (m)\" and ranges from 0 to 100 meters. The blue circles represent the pedestrian position, and the red crosses represent the distance between the vehicle and the pedestrian.\n\nThe scatter plot shows a distribution of data points across the entire range of episodes, with no clear trend or pattern. The pedestrian positions are scattered throughout the distance range, while the distance between the vehicle and pedestrian remains relatively constant, close to the 0-meter mark, indicating that the vehicle maintains a consistent distance from the pedestrian throughout the episodes.\n\nThe surrounding text appears to be related to a technical analysis or research study, possibly involving a neural network or a reinforcement learning algorithm, as indicated by terms like \"DQN,\" \"RMSProp,\" and \"training of DON.\" The text also mentions parameters such as \"learning rate,\" \"number of position data samples,\" \"size of the replay memory,\" and \"size of the trauma memory,\" which are likely related to the neural network's training process.\n\nThe context of the figure in relation to the text suggests that the scatter plot may be used to visualize the performance or behavior of a system, possibly in the context of autonomous driving or robotics, where maintaining a consistent distance from a pedestrian is crucial for safety. The figure could be illustrating the effectiveness of the system in maintaining this distance over a series of episodes.\n\nThe technical analysis of the figure would involve examining the distribution of pedestrian positions and the consistency of the distance between the vehicle and pedestrian. Key metrics or patterns that could be analyzed include the variance in pedestrian positions, the stability of the distance maintained, and any outliers or anomalies in the data.\n\nThe research impact of the figure could be significant if it demonstrates the effectiveness of the system in maintaining a safe distance from pedestrians, which could contribute to advancements in autonomous driving technology or robotics. The consistent distance maintained by the vehicle could indicate a successful implementation of a safety protocol or algorithm.",
          "context_found": true,
          "reference_text": "¢ Trigger point prrig = (5 — TTC) * vinis m\ne Safety line 1 = 3 m\ne AT=0.15\n\n© Ghigh; Imid; Glow, Anothing =\n\n{—9.8, —5.9, —2.9, 0} m/s?\n\nB. Training of DON\nThe neural network used for the DQN consists of the fully-\nconnected layers with five hidden layers. RMSProp algorithm\nis used to minimize the loss with learning rate 4 =\n0.0005. The number of position data samples used as a state is\nset to n = 5. We set the size of the replay memory to 10,000\nand that of the trauma memory to 1,000. We set t..."
        }
      ],
      "tables": [
        {
          "block": {
            "type": "table",
            "path": "results\\tables\\table_6.png",
            "y_position": 1648,
            "confidence": 0.17544405162334442
          },
          "coords": [
            126,
            1648,
            834,
            1732
          ],
          "file_path": "results\\tables\\table_6.png",
          "description": "The table presents data on two different scores, CVFA and CVNA, across various initial velocities (vinit) measured in kilometers per second (km/s). The initial velocities range from 20 km/s to 60 km/s, and the scores are numerical values that likely represent some form of assessment or rating at each velocity.\n\nHere is a summary of the key data points and findings from the table:\n\n- For both CVFA and CVNA scores, the values start at 1 for the lowest initial velocity of 20 km/s and increase to 3 for the highest initial velocities of 40 km/s and 45 km/s.\n- After reaching the peak score of 3, both scores decrease back to 1 at the highest initial velocity of 60 km/s.\n- The scores for CVFA and CVNA are identical at each initial velocity, suggesting that the two scores are related or that the assessment criteria for both are the same.\n\nThe surrounding text appears to be related to a different context, possibly discussing a neural network model for a DQN (Deep Q-Network) and its training parameters. It mentions a fully-connected layer with five hidden layers, the use of the RMSProp algorithm, a learning rate of 0.0005, and the number of position data samples used as a state set to 5. It also references a replay memory size of 10,000 and a trauma memory size of 1,000.\n\nThe text seems to be discussing the organization of data, critical findings, and research value, but without additional context, it's difficult to provide a detailed analysis of the findings or their significance.",
          "context_found": true,
          "reference_text": "¢ Trigger point prrig = (5 — TTC) * vinis m\ne Safety line 1 = 3 m\ne AT=0.15\n\n© Ghigh; Imid; Glow, Anothing =\n\n{—9.8, —5.9, —2.9, 0} m/s?\n\nB. Training of DON\nThe neural network used for the DQN consists of the fully-\nconnected layers with five hidden layers. RMSProp algorithm\nis used to minimize the loss with learning rate 4 =\n0.0005. The number of position data samples used as a state is\nset to n = 5. We set the size of the replay memory to 10,000\nand that of the trauma memory to 1,000. We set t..."
        }
      ],
      "text": "TABLE I\nCOLLISION RATE IN TEST SCENARIOS\n\nTTC (s) 0.9 TT 1.3 15 [1.7] 19\n\nCollision rate (%) 61.29 0.74 0 0 0\n\n2.1 7 2.3 [25 [2.7 [| 2.9 | 3.1 7 3.3 ] 3.5 [3.7 [39\n0 0 0 0 0 0 0 0 0 0\n\nproposed autonomous braking system exhibits consistent brake\ncontrol performance for all cases considered.\n\nFig. 7. Initial position of the pedestrian and relative distance between the\npedestrian and vehicle after the episode ends.\n\nD. Test Results for Euro NCAP AEB Pedestrian Test\n\nAdditional autonomous emergency braking (AEB) pedes-\ntrian tests are conducted. We follow the test procedure spec-\nified by Euro NCAP test protocol [16], [17]. Tests are con-\nducted for both farside (CVFA test) and nearside (CVNA test)\nunder the velocity range between 20 to 60 km/h with 5 km/h\ninterval. TTC is set to 4 s and the pedestrian crosses the road\nat 8 km/h for CVFA and 5 km/h for CVNA. Tests are scored\naccording to the rating parameters and the metric suggested in\nThe proposed system passed all tests without collision\nand the rating scores acquired by the proposed method are\nshown in Table II.\n\nTABLE II\nAEB TEST RESULT\n\nV. CONCLUSIONS\n\nWe have presented the new autonomous braking system\nbased on the deep reinforcement learning. The proposed\nsystem learns an intelligent way of brake control from the\nexperiences obtained under the simulated environment. We\ndesigned the autonomous braking systems using the DQN\n\nmethod with carefully designed reward function and enhanced\nstability of learning process by modifying the structure of\nthe DQN. We showed through computer simulations that the\nproposed autonomous braking system exhibits desirable and\nconsistent brake control behavior for various scenarios where\nbehavior of the pedestrian is uncertain.\n\nREFERENCES\n\n1] A. Vahidi and A. Eskandarian, Research advances in intelligent col-\nlision avodiance and adaptive cruise control,” JEEE Trans. Intelligent\nTransportation Systems, vol. 4, no. 3, pp. 143-153, Sept. 2003.\n\n2] K. Lee and H. Peng, \"Evaluation of automotive forward collision\nworning and collision avoidance algorithms,” Vehicle System Dynamics,\nvol. 43(10), pp. 735-751, Feb. 2007.\n3] T. Gandhi and M. M. Trivedi, ’Pedestrian collision avodiance systems: a\nsurvey of computer vision based recent studies,” JEEE Trans. Intelligent\nTransportation Systems Conference, pp. 976-981, Sept, 2006\n\n4] B. Huval, T. Wang, S. Tandon, J. Kiske, W. Song, J. Pazhayampallil,\nM. Andrilukam, P, Rajpurkar, T. Migimatsu, R. Cheng-Yue, F. Mujica,\nA. Coates, and A. Y. Ng, ”An empricial evaluatio of deep learning on\nhighway driving,” arXiv preprint, jarXiv:1504.01716) Apr. 2015.\n\n5] D. Tome, F. Monti, L. Baroffio, L. Bondi, M. Tagliasacchi and S. Tubaro,\nDeep convolutional neural networks for pedestrian detection,” Signal\nProcessing: Image Communication, vol. 47, pp. 482-489, May. 2016.\n\n6] R.S. Tomar and S. Verma, ’Neural network based lane change trajectory\nprediction in autonomous vehicles,” Transactions on computational\nscience XIII, Springer Berlin Heidelberg, pp. 125-146, 2011.\n\n7) M. Bojarski, D. D. Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal,\nL. D. Jackel, M. Monfort, U. Muller, J. Zhang, X. Zhang, J. Zhao and\nK. Zieba, ”End to end learning for self-driving cars,” arXiv preprint,\nApr. 2016.\n\n8] C. Chen, A. Seff, A. Kornhauser and J. Xiao, \"DeepDriving: learning\naffordance for direct perception in autonomous driving,” Proceedings\nof the IEEE International Conference on Computer Vision(ICCV), pp.\n2722-2730, 2015.\n\n9] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S.\nPetersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D.\nWierstra, S. Legg and D. Hassabis, *\"Human-level control through deep\nreinforcement learning,” Nature, vol. 518(7540), pp. 529-533,2015.\n\n10] J. Koutnik, J. Schmidhuber and F. Gomez, ’Evolving deep unsupervised\nconvolutional networks for vision-based reinforcement learning,” Pro-\nceedings of the 2014 Annual Conference on Genetic and Evolutionary\nComputation, ACM, pp. 541-548, 2014.\n\n11] C. Desjardins and B. Chaib-draa, Cooperative adaptive cruise control: a\nreinforcement learning approch,” JEEE Trans. Intelligent Transportation\nSystems, vol. 12, no. 4, pp. 1248-1260 Dec. 2011.\n\n12] R.S. Sutton and A. G. Barto, ’Reinforcement learning: an introduction,”,\nMIT Press, 1998\n\n13] V. Nair and G. E. Hinton, ”Rectified linear units improve restricted\nboltzmann machines,” Proceedings of the 27th Internation Conference\non Machine Learning(ICML), pp. 807-814, 2010.\n\n14] T. Tieleman and G. Hinton, Lecture 6.5-rmsprop: Devide the gradient\nby a running average of its recent magnitude,” COURSERA: Neural\nnetwork for machine learning, 4.2, 2012.\n\n15] PreScan, https://www.tassinternational.com/prescan\n\n16] Pedestrian testing protocol, European New Car Assessment Programme,\nDec. 2016.\n\n17] P. seiniger, A. Hellmann, O. Bartels, M. Wisch and J. Gail Test\nProcedures and Results for Pedestrian AEB Systems,” Proceedings of\nthe 24th Conference on the Enhancement of the Safety of Vehicles(ESV)\n» 2015.\n\n"
    }
  ]
}